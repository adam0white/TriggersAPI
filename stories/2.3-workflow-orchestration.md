---
title: "Epic 2.3 - Workflow Implementation: Multi-Step Orchestration with Retries"
status: "Ready for Development"
epic: "Epic 2: Event Processing & Storage + Metrics Display"
priority: "P0"
acceptance_criteria:
  - "Workflow handles 3-step processing: validate → store → update metrics"
  - "Step 1 (Validate): Check event payload structure and required fields"
  - "Step 2 (Store): Write event to D1 database with status='pending'"
  - "Step 3 (Metrics): Update KV counters for total events and status distribution"
  - "Workflow retries on failure for each step independently"
  - "Durable execution: Workflow state persists across Worker restarts"
  - "Error handling: Log failures with correlation ID and error details"
  - "Timeout: Workflow completes within 30 seconds end-to-end"
  - "Dead Letter Queue routing: Failed workflows after retries send to DLQ"
  - "Workflow input/output typed with TypeScript interfaces"
  - "Integration: Queue consumer triggers workflow for each event"
  - "State transitions: Track pending → delivered status through workflow steps"
  - "Concurrent workflows: Support 100+ concurrent workflow executions"
  - "Workflow idempotency: Same event_id reprocessed produces same result"
  - "Performance: Process 1000 events through workflow in < 15 seconds"
created_at: "2025-11-10"
modified_at: "2025-11-10"
story_size: "Large"
depends_on: "Epic 1.1 - Project Setup, Epic 2.1 - D1 Schema, Epic 2.2 - Queue Consumer"
---

## Summary

Implement Cloudflare Workflows to orchestrate guaranteed multi-step event processing: validation → D1 storage → KV metrics update. Workflows provide durable, retriable execution guarantees for the critical processing pipeline.

## Business Value

Guarantees event processing durability. Without workflows, system lacks guaranteed execution semantics - events could be lost on Worker restarts or process failures. Workflows ensure every queued event eventually either succeeds or explicitly fails to DLQ.

## Technical Requirements

### Workflow Architecture (from PRD - FR-2.2)

**Workflow Purpose:** Guaranteed, retriable multi-step orchestration

**Three-Step Pipeline:**

1. **Validation:** Verify event_id, payload, metadata structure
2. **Storage:** Write to D1 with status=pending
3. **Metrics:** Increment KV counters (total, pending count)

**Failure Handling:**
- Each step can retry independently
- Failed workflow routes to DLQ after max retries
- DLQ messages are persisted for manual inspection

### Workflow Configuration (from wrangler.toml)

Already configured in Epic 1.1 (now uncommented for Epic 2.3):

```toml
[[workflows]]
name = "process-event"
binding = "PROCESS_EVENT_WORKFLOW"
```

### Workflow Implementation

**File Location:** `src/workflows/process-event.ts`

**Workflow Definition:**

```typescript
import { WorkflowEntrypoint, WorkflowStep, WorkflowEvent } from '@cloudflare/workers-types';
import { Event, CreateEventInput } from '../types/events';
import { logger } from '../lib/logger';

export interface ProcessEventInput {
  event_id: string;
  payload: Record<string, any>;
  metadata?: Record<string, any>;
  timestamp: string;
  correlation_id: string;
  retry_attempt: number;
}

export interface ProcessEventOutput {
  event_id: string;
  status: 'success' | 'failure';
  stored_at?: string;
  error?: string;
}

export class ProcessEventWorkflow extends WorkflowEntrypoint<Env, ProcessEventInput> {
  async run(
    event: WorkflowEvent<ProcessEventInput>,
    step: WorkflowStep,
    env: Env,
  ): Promise<ProcessEventOutput> {
    const input = event.payload;
    const { event_id, payload, metadata, timestamp, correlation_id, retry_attempt } = input;

    logger.info('Workflow started', {
      correlation_id,
      event_id,
      workflow_id: event.id,
      retry_attempt,
    });

    try {
      // Step 1: Validate event
      const validated = await step.do(
        'validate-event',
        async () => {
          logger.debug('Validating event', {
            correlation_id,
            event_id,
          });

          // Validation logic
          if (!event_id || typeof event_id !== 'string') {
            throw new Error('Invalid event_id');
          }

          if (!payload || typeof payload !== 'object') {
            throw new Error('Invalid payload');
          }

          // Metadata is optional
          if (metadata && typeof metadata !== 'object') {
            throw new Error('Invalid metadata');
          }

          logger.info('Event validation passed', {
            correlation_id,
            event_id,
          });

          return {
            event_id,
            payload,
            metadata,
            timestamp,
            correlation_id,
          };
        },
      );

      // Step 2: Store event in D1
      const stored = await step.do(
        'store-event',
        async () => {
          logger.debug('Storing event to D1', {
            correlation_id,
            event_id,
          });

          const db = env.DB;
          const now = new Date().toISOString();

          const result = await db
            .prepare(`
              INSERT INTO events (
                event_id, payload, metadata, status, created_at, updated_at, retry_count
              ) VALUES (?, ?, ?, ?, ?, ?, ?)
            `)
            .bind(
              event_id,
              JSON.stringify(payload),
              metadata ? JSON.stringify(metadata) : null,
              'pending',
              timestamp,
              now,
              retry_attempt,
            )
            .run();

          logger.info('Event stored in D1', {
            correlation_id,
            event_id,
            storage_time: now,
          });

          return {
            event_id,
            status: 'pending' as const,
            stored_at: now,
          };
        },
      );

      // Step 3: Update metrics in KV
      const metrics = await step.do(
        'update-metrics',
        async () => {
          logger.debug('Updating metrics in KV', {
            correlation_id,
            event_id,
          });

          const kv = env.AUTH_KV; // Use same KV for metrics

          // Increment counters atomically
          await Promise.all([
            // Increment total events
            incrementKVCounter(kv, 'metrics:events:total'),

            // Increment pending count
            incrementKVCounter(kv, 'metrics:events:pending'),

            // Update last processed timestamp
            kv.put('metrics:last_processed_at', new Date().toISOString()),
          ]);

          logger.info('Metrics updated', {
            correlation_id,
            event_id,
          });

          return {
            total_updated: true,
            pending_updated: true,
          };
        },
      );

      logger.info('Workflow completed successfully', {
        correlation_id,
        event_id,
        workflow_id: event.id,
        duration_ms: Date.now() - new Date(timestamp).getTime(),
      });

      return {
        event_id,
        status: 'success',
        stored_at: stored.stored_at,
      };
    } catch (error) {
      logger.error('Workflow failed', {
        correlation_id,
        event_id,
        error: error instanceof Error ? error.message : 'Unknown error',
        retry_attempt,
      });

      return {
        event_id,
        status: 'failure',
        error: error instanceof Error ? error.message : 'Unknown error',
      };
    }
  }
}

// Helper: Increment KV counter with atomic semantics
async function incrementKVCounter(kv: KVNamespace, key: string): Promise<void> {
  const current = await kv.get(key);
  const count = current ? parseInt(current, 10) : 0;
  await kv.put(key, String(count + 1));
}

export default new ProcessEventWorkflow();
```

### Workflow Invocation from Queue Consumer

**Update:** `src/queue/consumer.ts`

Modify `processMessage()` to invoke workflow:

```typescript
async function processMessage(
  message: Message<QueueMessage>,
  env: Env,
  batchId: string,
): Promise<void> {
  const { event_id, payload, metadata, timestamp, correlation_id } = message.body;
  const correlationId = correlation_id || crypto.randomUUID();
  const retryCount = message.retryCount || 0;

  logger.info('Triggering workflow for event', {
    correlation_id: correlationId,
    event_id,
    retry_attempt: retryCount,
    batch_id: batchId,
  });

  try {
    // Invoke workflow with event data
    const workflowRun = await env.PROCESS_EVENT_WORKFLOW.create({
      id: `event-${event_id}-${batchId}`,
      params: {
        event_id,
        payload,
        metadata,
        timestamp,
        correlation_id: correlationId,
        retry_attempt: retryCount,
      } as ProcessEventInput,
    });

    logger.info('Workflow created', {
      correlation_id: correlationId,
      event_id,
      workflow_id: workflowRun.id,
    });

    // Optionally wait for workflow completion (for synchronous processing)
    // Note: Workflows are durable so we don't need to wait
    // const result = await workflowRun.result();
  } catch (error) {
    logger.error('Failed to create workflow', {
      correlation_id: correlationId,
      event_id,
      error: error instanceof Error ? error.message : 'Unknown',
    });
    throw error; // Trigger queue retry
  }
}
```

### Types and Interfaces

**Update:** `src/types/env.ts`

```typescript
import {
  D1Database,
  KVNamespace,
  Queue,
  Workflow
} from '@cloudflare/workers-types';

export interface Env {
  DB: D1Database;
  AUTH_KV: KVNamespace;
  EVENT_QUEUE: Queue<unknown>;
  PROCESS_EVENT_WORKFLOW: Workflow<ProcessEventInput>;
  Environment: 'development' | 'production';
  LOG_LEVEL: 'debug' | 'info' | 'warn' | 'error';
}
```

### Error Handling and Retries

**Workflow Retry Semantics (from Cloudflare Docs):**

1. **Automatic Retries:** Each step retried up to configured max (default: 3)
2. **Exponential Backoff:** Delay between retries increases exponentially
3. **DLQ Routing:** After max retries, workflow state persists
4. **Idempotency:** Same workflow ID with same input produces same result

**Implementation Pattern:**

```typescript
// Step 1: Validation can fail immediately
// Step 2: Database write can fail on constraint or connectivity
// Step 3: KV write can fail on quota

// Each step retried independently
// If all steps eventually succeed: event processed
// If step exceeds max retries: workflow failure → DLQ

// Idempotency: If workflow retried with same event_id:
// - Database UPSERT or INSERT IF NOT EXISTS
// - KV increment is idempotent (always increments by 1)
```

### Metrics KV Structure

**Metrics Keys:**

```typescript
// Aggregate counters
'metrics:events:total'      // Total events processed
'metrics:events:pending'    // Events with status=pending
'metrics:events:delivered'  // Events with status=delivered
'metrics:events:failed'     // Events with status=failed

// Performance metrics
'metrics:last_processed_at' // ISO-8601 timestamp
'metrics:queue:depth'       // Current queue depth (updated by queue consumer)
'metrics:dlq:count'         // Messages in DLQ
```

**Counter Increment Implementation:**

```typescript
async function incrementKVCounter(kv: KVNamespace, key: string, delta: number = 1): Promise<number> {
  // KV doesn't support atomic increment, so we do: read → increment → write
  const current = await kv.get(key, 'text');
  const count = current ? parseInt(current, 10) : 0;
  const newCount = count + delta;
  await kv.put(key, String(newCount), {
    metadata: {
      updated_at: new Date().toISOString(),
    },
  });
  return newCount;
}
```

### Performance Considerations

**Latency Targets (from PRD - NFR-1):**
- Workflow execution: < 10 seconds end-to-end
- Database write: < 100ms
- KV update: < 50ms
- Queue to processing start: < 5 seconds

**Scaling Considerations:**
- Cloudflare Workflows auto-scale
- Support 100+ concurrent executions
- D1 single-writer (but multiple readers via replicas)
- KV eventual consistency acceptable for metrics

**Optimization Strategies:**
- Parallel step execution where possible
- Batch KV counter updates (combine multiple increments)
- D1 transaction boundaries to ensure atomicity

### Testing and Verification

**Local Testing with Wrangler:**

```bash
# Start Wrangler with workflow support
npx wrangler dev

# Monitor workflow execution via console logs
# Verify: validation → storage → metrics steps

# For workflow debugging:
# - Check console for step execution logs
# - Verify D1 table has new events
# - Inspect KV metrics values
```

**Testing Steps:**

1. **Trigger Event Processing:**
   ```bash
   curl -X POST http://localhost:8787/events \
     -H "Authorization: Bearer test-token" \
     -H "Content-Type: application/json" \
     -d '{"payload":{"test":"data"}}'
   ```

2. **Monitor Workflow Execution:**
   - Watch console for "Workflow started" log
   - Verify each step completes
   - Check for errors in step execution

3. **Verify D1 Storage:**
   ```bash
   npx wrangler d1 execute triggers-api --local
   SELECT * FROM events;
   ```

4. **Verify KV Metrics:**
   ```bash
   npx wrangler kv:key list --namespace-id=<id> --local
   # Should show metrics:events:total, metrics:events:pending
   ```

5. **Test Error Path:**
   - Send invalid event (missing payload)
   - Verify validation step fails
   - Check error logging

6. **Test Retry:**
   - Simulate database failure (via debug flag in later epic)
   - Verify workflow retries
   - Check retry count in logs

### Cloudflare Workflows Documentation

**Key Concepts:**
- Durable execution engine with guaranteed semantics
- Steps can call external services or databases
- Automatic retry with exponential backoff
- Workflow state persisted across restarts
- Step results available to subsequent steps

**Deployment:**
- Workflows deployed with Worker
- Callable from any Worker via binding
- No separate deployment needed

---

## Implementation Notes

### What Gets Done

1. Create `src/workflows/process-event.ts` with ProcessEventWorkflow class
2. Define ProcessEventInput and ProcessEventOutput interfaces
3. Implement 3-step workflow: validate → store → metrics
4. Create helper: incrementKVCounter() for atomic-like operations
5. Update `src/queue/consumer.ts` to invoke workflow
6. Update `src/types/env.ts` with Workflow binding
7. Uncomment workflow binding in wrangler.toml
8. Create test file: `test/workflows/process-event.test.ts`
9. Test locally: Send events and verify workflow execution
10. Commit: `git add src/workflows/ && git commit -m "feat: workflow orchestration for event processing"`

### Development Workflow

1. Ensure wrangler.toml has workflow binding
2. Start: `npx wrangler dev`
3. Send test event via curl or API
4. Monitor console for workflow logs
5. Verify D1 has new events
6. Verify KV metrics incremented
7. Test error cases (invalid data)
8. Monitor workflow retries

### Key Architecture Decisions

**Three-Step Pipeline:** Separation of concerns for validation, storage, metrics

**Durable Execution:** Workflows guarantee completion (vs manual retry logic)

**Idempotent Operations:** Same event_id produces same result on retry

**KV Metrics:** Real-time counters for dashboard consumption

---

## Acceptance Criteria Verification Checklist

### Workflow Structure
- [ ] ProcessEventWorkflow extends WorkflowEntrypoint
- [ ] Workflow handles ProcessEventInput interface
- [ ] Three steps implemented: validate, store, metrics
- [ ] Workflow registered as default export

### Validation Step
- [ ] event_id validation: must be non-empty string
- [ ] payload validation: must be object
- [ ] metadata validation: optional, must be object if present
- [ ] Validation errors logged with details
- [ ] Validation step can retry

### Storage Step
- [ ] D1 INSERT executed with correct fields
- [ ] Event stored with status='pending'
- [ ] retry_count field populated
- [ ] created_at and updated_at set correctly
- [ ] Storage errors logged
- [ ] Storage step can retry

### Metrics Step
- [ ] KV counter 'metrics:events:total' incremented
- [ ] KV counter 'metrics:events:pending' incremented
- [ ] Last processed timestamp updated
- [ ] Metrics errors logged
- [ ] Metrics step can retry

### Error Handling
- [ ] Step failures logged with correlation_id
- [ ] Workflow returns error in ProcessEventOutput
- [ ] Failed workflows route to DLQ (via Cloudflare)
- [ ] Workflow ID unique per event-batch combination

### Performance
- [ ] Workflow completes in < 30 seconds
- [ ] 100+ concurrent workflows supported
- [ ] 1000 events processed in < 15 seconds
- [ ] No blocking operations in workflow

### Idempotency
- [ ] Same event_id reprocessed produces same result
- [ ] Database doesn't error on duplicate event_id
- [ ] KV counters increment correctly on retries

### Integration
- [ ] Queue consumer invokes workflow successfully
- [ ] Workflow runs after message ack
- [ ] Workflow ID correlates to event_id and batch ID
- [ ] Correlation ID propagated to workflow logs

### Local Development
- [ ] `npx wrangler dev` recognizes workflow binding
- [ ] Workflow execution logs appear in console
- [ ] Workflow completion confirmed by D1/KV updates
- [ ] Multiple events trigger multiple workflows

---

## Dependencies & Context

**From:** docs/PRD.md (Epic 2 section - Workflow Orchestration FR-2.2)
**Architecture:** docs/architecture.md (Implementation Patterns - Multi-step orchestration)
**Depends On:** Epic 1.1 (Project Setup), Epic 2.1 (D1 Schema), Epic 2.2 (Queue Consumer)
**Enables:** Epic 2.4 (Event Storage), Epic 2.5 (Metrics Updates)

**Cloudflare Documentation:**
- [Workflows Guide](https://developers.cloudflare.com/workflows/)
- [Step Execution](https://developers.cloudflare.com/workflows/build/steps/)
- [Error Handling](https://developers.cloudflare.com/workflows/manage/error-handling/)

---

## Dev Notes

- Workflow IDs must be unique but deterministic (include event_id and batch ID)
- Steps are not necessarily parallel - they execute in sequence
- Workflow state automatically persisted by Cloudflare
- DLQ is inspected via Cloudflare dashboard (not accessible via API in MVP)
- KV operations in workflow run inside Worker context (same as regular code)
- correlation_id is critical for tracing - maintain it through all steps

---

## DLQ Inspection (Post-Deployment)

After deployment to Cloudflare:

1. Login to Cloudflare dashboard
2. Navigate to Workers → Queues
3. Check "event-dlq" for dead-lettered messages
4. Inspect message body and retry attempts
5. Optionally replay from DLQ (manual process)

For MVP, DLQ inspection is manual. Growth feature: API endpoint to query DLQ.

---
