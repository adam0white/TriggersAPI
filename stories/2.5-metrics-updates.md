---
title: "Epic 2.5 - Metrics Updates: KV Aggregate Counters and DLQ Routing"
status: "Ready for Development"
epic: "Epic 2: Event Processing & Storage + Metrics Display"
priority: "P0"
acceptance_criteria:
  - "Workflow step 3 updates KV counters for metrics tracking"
  - "metrics:events:total incremented on every successful storage"
  - "metrics:events:pending incremented on storage (status=pending)"
  - "metrics:events:delivered incremented when event status changes to delivered"
  - "metrics:events:failed incremented when event status changes to failed"
  - "metrics:queue:depth tracking current queue message count"
  - "metrics:dlq:count tracking messages in Dead Letter Queue"
  - "metrics:last_processed_at updated with ISO-8601 timestamp"
  - "KV counter updates are atomic operations (read-modify-write)"
  - "Performance: KV metric updates complete within 50ms"
  - "No race conditions: Concurrent metric updates handled correctly"
  - "DLQ messages routed after workflow max_retries exceeded"
  - "Failed workflow events logged with correlation_id for DLQ inspection"
  - "Metrics queryable via API (used by UI in Epic 2.6)"
  - "All metrics keys use consistent 'metrics:' prefix for organization"
created_at: "2025-11-10"
modified_at: "2025-11-10"
story_size: "Medium"
depends_on: "Epic 1.1 - Project Setup, Epic 2.3 - Workflow, Epic 2.4 - Event Storage"
---

## Summary

Implement the metrics update step in the workflow. This story covers real-time KV counter updates that track event flow through the system, enabling the metrics dashboard to display live statistics.

## Business Value

Enables observability and monitoring. Metrics allow operators to see system health in real-time: how many events flowing, what percentage pending/delivered/failed, queue depth, etc. Without metrics, system behavior is invisible.

## Technical Requirements

### KV Metrics Structure

**File Location:** `src/lib/metrics.ts`

**Metrics Keys and Purpose:**

```typescript
// Aggregate counters - incremented as events flow through system
'metrics:events:total'      // Total events ever processed
'metrics:events:pending'    // Events with status='pending'
'metrics:events:delivered'  // Events with status='delivered'
'metrics:events:failed'     // Events with status='failed'

// Queue monitoring
'metrics:queue:depth'       // Current messages in event queue
'metrics:dlq:count'         // Messages in dead-letter queue

// Timestamps
'metrics:last_processed_at' // When was last event processed (ISO-8601)
'metrics:last_failure_at'   // When was last failure (ISO-8601)

// Performance
'metrics:processing_time:p50'  // Median processing time in ms
'metrics:processing_time:p95'  // 95th percentile
'metrics:processing_time:p99'  // 99th percentile
```

### Metrics Helper Functions

**Create:** `src/lib/metrics.ts`

```typescript
import { KVNamespace } from '@cloudflare/workers-types';
import { logger } from './logger';

export interface Metrics {
  total_events: number;
  pending: number;
  delivered: number;
  failed: number;
  queue_depth: number;
  dlq_count: number;
  last_processed_at: string | null;
  processing_rate: number; // events per minute estimate
}

export class MetricsManager {
  constructor(private kv: KVNamespace) {}

  /**
   * Increment a counter atomically
   * Reads current value, increments, writes back
   * Note: KV doesn't have true atomic increment, but acceptable for metrics
   */
  async incrementCounter(key: string, delta: number = 1, metadata?: Record<string, any>): Promise<number> {
    try {
      const current = await this.kv.get(key, 'text');
      const currentValue = current ? parseInt(current, 10) : 0;
      const newValue = currentValue + delta;

      await this.kv.put(key, String(newValue), {
        metadata: {
          updated_at: new Date().toISOString(),
          ...metadata,
        },
      });

      return newValue;
    } catch (error) {
      logger.error('Failed to increment counter', {
        key,
        delta,
        error: error instanceof Error ? error.message : 'Unknown',
      });
      throw error;
    }
  }

  /**
   * Update metrics on event storage
   * Called from workflow step 3
   */
  async recordEventStored(
    eventId: string,
    status: 'pending' | 'delivered' | 'failed',
    processingTimeMs: number,
  ): Promise<void> {
    try {
      // Increment total and status-specific counters
      await Promise.all([
        this.incrementCounter('metrics:events:total', 1, {
          event_id: eventId,
        }),
        this.incrementCounter(`metrics:events:${status}`, 1),
        this.updateLastProcessedAt(),
        this.recordProcessingTime(processingTimeMs),
      ]);

      logger.info('Metrics recorded for event storage', {
        event_id: eventId,
        status,
        processing_time_ms: processingTimeMs,
      });
    } catch (error) {
      logger.error('Failed to record metrics', {
        event_id: eventId,
        error: error instanceof Error ? error.message : 'Unknown',
      });
      // Don't fail workflow if metrics update fails
      // Metrics are secondary to core functionality
    }
  }

  /**
   * Update metrics on status change
   * Called when event transitions pending → delivered/failed
   */
  async recordStatusChange(
    eventId: string,
    previousStatus: 'pending' | 'delivered' | 'failed',
    newStatus: 'pending' | 'delivered' | 'failed',
  ): Promise<void> {
    try {
      // Decrement old status counter
      const oldCount = await this.kv.get(`metrics:events:${previousStatus}`, 'text');
      if (oldCount) {
        const currentValue = parseInt(oldCount, 10);
        await this.kv.put(`metrics:events:${previousStatus}`, String(Math.max(0, currentValue - 1)));
      }

      // Increment new status counter
      await this.incrementCounter(`metrics:events:${newStatus}`, 1);

      logger.info('Metrics recorded for status change', {
        event_id: eventId,
        from: previousStatus,
        to: newStatus,
      });
    } catch (error) {
      logger.error('Failed to record status change metrics', {
        event_id: eventId,
        error: error instanceof Error ? error.message : 'Unknown',
      });
    }
  }

  /**
   * Record processing failure
   * Called when workflow fails or event routed to DLQ
   */
  async recordFailure(
    eventId: string,
    reason: string,
    correlationId: string,
  ): Promise<void> {
    try {
      await Promise.all([
        this.incrementCounter('metrics:events:failed', 1),
        this.updateLastFailureAt(),
        this.kv.put(`dlq:${eventId}`, JSON.stringify({
          event_id: eventId,
          reason,
          correlation_id: correlationId,
          failed_at: new Date().toISOString(),
        })),
      ]);

      logger.info('Failure recorded', {
        event_id: eventId,
        reason,
        correlation_id: correlationId,
      });
    } catch (error) {
      logger.error('Failed to record failure metrics', {
        event_id: eventId,
        error: error instanceof Error ? error.message : 'Unknown',
      });
    }
  }

  /**
   * Update queue depth metric
   * Called by queue consumer when batch received
   */
  async updateQueueDepth(depth: number): Promise<void> {
    try {
      await this.kv.put('metrics:queue:depth', String(depth), {
        metadata: {
          updated_at: new Date().toISOString(),
        },
      });
    } catch (error) {
      logger.error('Failed to update queue depth', {
        error: error instanceof Error ? error.message : 'Unknown',
      });
    }
  }

  /**
   * Update DLQ message count
   * (Manual in MVP, could be automated post-deployment)
   */
  async updateDLQCount(count: number): Promise<void> {
    try {
      await this.kv.put('metrics:dlq:count', String(count), {
        metadata: {
          updated_at: new Date().toISOString(),
        },
      });
    } catch (error) {
      logger.error('Failed to update DLQ count', {
        error: error instanceof Error ? error.message : 'Unknown',
      });
    }
  }

  /**
   * Get all metrics for dashboard
   */
  async getAllMetrics(): Promise<Metrics> {
    try {
      const [total, pending, delivered, failed, queueDepth, dlqCount, lastProcessed] =
        await Promise.all([
          this.kv.get('metrics:events:total', 'text'),
          this.kv.get('metrics:events:pending', 'text'),
          this.kv.get('metrics:events:delivered', 'text'),
          this.kv.get('metrics:events:failed', 'text'),
          this.kv.get('metrics:queue:depth', 'text'),
          this.kv.get('metrics:dlq:count', 'text'),
          this.kv.get('metrics:last_processed_at', 'text'),
        ]);

      return {
        total_events: total ? parseInt(total, 10) : 0,
        pending: pending ? parseInt(pending, 10) : 0,
        delivered: delivered ? parseInt(delivered, 10) : 0,
        failed: failed ? parseInt(failed, 10) : 0,
        queue_depth: queueDepth ? parseInt(queueDepth, 10) : 0,
        dlq_count: dlqCount ? parseInt(dlqCount, 10) : 0,
        last_processed_at: lastProcessed || null,
        processing_rate: this.calculateProcessingRate(
          total ? parseInt(total, 10) : 0,
          lastProcessed
        ),
      };
    } catch (error) {
      logger.error('Failed to retrieve metrics', {
        error: error instanceof Error ? error.message : 'Unknown',
      });
      throw error;
    }
  }

  /**
   * Reset metrics to zero (dev/testing only)
   */
  async resetMetrics(correlationId: string): Promise<void> {
    const keys = [
      'metrics:events:total',
      'metrics:events:pending',
      'metrics:events:delivered',
      'metrics:events:failed',
      'metrics:queue:depth',
      'metrics:dlq:count',
    ];

    try {
      await Promise.all(keys.map(key => this.kv.put(key, '0')));
      logger.info('Metrics reset', { correlation_id: correlationId });
    } catch (error) {
      logger.error('Failed to reset metrics', {
        error: error instanceof Error ? error.message : 'Unknown',
      });
    }
  }

  /**
   * Internal: Update last processed timestamp
   */
  private async updateLastProcessedAt(): Promise<void> {
    await this.kv.put('metrics:last_processed_at', new Date().toISOString());
  }

  /**
   * Internal: Update last failure timestamp
   */
  private async updateLastFailureAt(): Promise<void> {
    await this.kv.put('metrics:last_failure_at', new Date().toISOString());
  }

  /**
   * Internal: Record processing time for latency calculation
   */
  private async recordProcessingTime(ms: number): Promise<void> {
    // Store last processing time (could be enhanced with percentile calculation)
    await this.kv.put('metrics:last_processing_time_ms', String(ms));
  }

  /**
   * Internal: Calculate events per minute from total and timestamp
   */
  private calculateProcessingRate(totalEvents: number, lastProcessedAt: string | null): number {
    if (!lastProcessedAt || totalEvents === 0) return 0;

    const lastProcessed = new Date(lastProcessedAt).getTime();
    const now = Date.now();
    const elapsedMinutes = (now - lastProcessed) / (1000 * 60);

    if (elapsedMinutes === 0) return 0;

    return Math.round(totalEvents / elapsedMinutes);
  }
}
```

### Workflow Integration (Update to 2.3)

**Update:** `src/workflows/process-event.ts`

Implement step 3 using MetricsManager:

```typescript
// Step 3: Update metrics in KV
const metrics = await step.do(
  'update-metrics',
  async () => {
    logger.debug('Updating metrics', {
      correlation_id: correlationId,
      event_id,
    });

    try {
      const metricsManager = new MetricsManager(env.AUTH_KV);

      // Record event storage with processing time
      const processingTimeMs = Date.now() - new Date(timestamp).getTime();

      await metricsManager.recordEventStored(
        event_id,
        'pending', // New events always start as pending
        processingTimeMs,
      );

      logger.info('Metrics updated successfully', {
        correlation_id: correlationId,
        event_id,
        processing_time_ms: processingTimeMs,
      });

      return {
        total_updated: true,
        metrics_recorded: true,
      };
    } catch (error) {
      // Log but don't fail - metrics are secondary
      logger.warn('Metrics update failed but continuing', {
        correlation_id: correlationId,
        event_id,
        error: error instanceof Error ? error.message : 'Unknown',
      });

      return {
        total_updated: false,
        metrics_recorded: false,
      };
    }
  },
);
```

### API Endpoint for Metrics

**Create:** `src/routes/metrics.ts`

Expose metrics via GET /metrics endpoint:

```typescript
import { MetricsManager } from '../lib/metrics';

export async function getMetrics(
  request: Request,
  env: Env,
): Promise<Response> {
  try {
    const metricsManager = new MetricsManager(env.AUTH_KV);
    const metrics = await metricsManager.getAllMetrics();

    return new Response(
      JSON.stringify({
        data: metrics,
        timestamp: new Date().toISOString(),
      }),
      {
        status: 200,
        headers: { 'Content-Type': 'application/json' },
      },
    );
  } catch (error) {
    return new Response(
      JSON.stringify({
        error: {
          code: 'METRICS_UNAVAILABLE',
          message: 'Failed to retrieve metrics',
          timestamp: new Date().toISOString(),
        },
      }),
      {
        status: 500,
        headers: { 'Content-Type': 'application/json' },
      },
    );
  }
}
```

### Dead Letter Queue Handling

**DLQ Routing Flow:**

```
Event Processing Failure
        ↓
Workflow Step Fails
        ↓
Workflow Retry (up to 3x)
        ↓
All Retries Exhausted
        ↓
Message Routes to DLQ (Cloudflare automatic)
        ↓
DLQ Message Stored (preserved for inspection)
        ↓
Failure Logged with correlation_id (for tracing)
        ↓
KV metric 'metrics:dlq:count' incremented
        ↓
Dashboard displays DLQ count
```

**Queue Consumer Handling (from 2.2):**

```typescript
// In processEventBatch():
results.forEach((result, index) => {
  if (result.status === 'fulfilled') {
    batch.messages[index].ack(); // Success
  } else {
    // Don't ack failed message
    // Cloudflare Queues handles: retry → DLQ
    logger.error('Message processing failed', {
      message_index: index,
      error: result.reason,
    });
  }
});
```

### DLQ Inspection (Post-Deployment)

**For Development:**

```typescript
// Store failed event metadata in KV for inspection
await metricsManager.recordFailure(
  eventId,
  'Workflow failed after 3 retries',
  correlationId,
);

// Key: dlq:<event_id>
// Value: { event_id, reason, correlation_id, failed_at }
```

**For Production (Growth Feature):**
- Cloudflare dashboard: Queues → event-dlq → View Messages
- API endpoint: GET /dlq - List failed messages
- Replay endpoint: POST /dlq/:id/replay - Requeue message

### Performance Considerations

**KV Operation Latency:**
- Write: ~1-5ms per operation
- Read: ~1-5ms per operation
- Bulk read (all metrics): ~10-20ms for 10 keys

**Optimization Strategy:**
- Batch counter updates (combine multiple increments)
- Use Promise.all for parallel KV operations
- Cache metrics locally if needed (growth feature)

**Race Condition Handling:**

```typescript
// Multiple workflows updating same counter concurrently
// Scenario: Workflow A and B both try to increment metrics:events:total

// Sequence:
// A: read current value (100) → increment to 101
// B: read current value (100) → increment to 101
// A: write 101
// B: write 101

// Result: Counter is 101 instead of 102 (lost update)
// This is acceptable for metrics (approximation is fine)
// For critical counters, would need distributed lock
```

### Testing Verification

**Manual Testing:**

```bash
# Start Wrangler
npx wrangler dev

# Send event
curl -X POST http://localhost:8787/events \
  -H "Authorization: Bearer test-token" \
  -H "Content-Type: application/json" \
  -d '{"payload":{"test":"data"}}'

# Check metrics
curl http://localhost:8787/metrics \
  -H "Authorization: Bearer test-token"

# Expected response:
{
  "data": {
    "total_events": 1,
    "pending": 1,
    "delivered": 0,
    "failed": 0,
    "queue_depth": 0,
    "dlq_count": 0,
    "last_processed_at": "2025-11-10T12:34:56.789Z",
    "processing_rate": 60
  },
  "timestamp": "2025-11-10T12:34:57.000Z"
}
```

**Load Testing:**

```bash
# Send 100 events
for i in {1..100}; do
  curl -X POST http://localhost:8787/events \
    -H "Authorization: Bearer test-token" \
    -H "Content-Type: application/json" \
    -d "{\"payload\":{\"id\":$i}}" &
done
wait

# Check metrics - should show:
# total_events: 100
# pending: 100
# processing_time: < 100ms per event
```

---

## Implementation Notes

### What Gets Done

1. Create `src/lib/metrics.ts` with MetricsManager class
2. Implement counter increment function (atomic-like read-modify-write)
3. Implement recordEventStored(), recordStatusChange(), recordFailure()
4. Implement getAllMetrics() for dashboard consumption
5. Implement updateQueueDepth() and updateDLQCount()
6. Create `src/routes/metrics.ts` with GET /metrics endpoint
7. Update workflow step 3 to use MetricsManager
8. Create test file: `test/lib/metrics.test.ts`
9. Update src/index.ts to add metrics endpoint
10. Test locally: Send events and verify metrics update
11. Commit: `git add src/lib/metrics.ts src/routes/metrics.ts && git commit -m "feat: KV metrics tracking"`

### Development Workflow

1. Ensure workflow step 2 (storage) complete
2. Start: `npx wrangler dev`
3. Send events and check metrics endpoint
4. Monitor console for metric update logs
5. Load test: 100+ events
6. Verify DLQ tracking (manual post-deployment)

### Key Architecture Decisions

**KV for Metrics:** Fast, distributed, eventual consistency acceptable

**Counters as Separate Keys:** Allows independent updates, simple to query

**Non-Blocking Failures:** Metrics failures don't block workflow

**Correlation ID Tracking:** Enable tracing through DLQ

---

## Acceptance Criteria Verification Checklist

### Counter Operations
- [ ] incrementCounter() implements read-modify-write
- [ ] Metadata tracked with counter updates
- [ ] Delta parameter allows increments > 1
- [ ] All counter operations logged

### Event Storage Metrics
- [ ] recordEventStored() increments metrics:events:total
- [ ] recordEventStored() increments metrics:events:pending
- [ ] recordEventStored() updates last_processed_at
- [ ] Processing time recorded for latency calculation

### Status Change Metrics
- [ ] recordStatusChange() decrements old status counter
- [ ] recordStatusChange() increments new status counter
- [ ] No negative counts (Math.max(0, value - 1))
- [ ] Logged with both old and new status

### Failure Metrics
- [ ] recordFailure() increments metrics:events:failed
- [ ] recordFailure() updates last_failure_at
- [ ] Failed event metadata stored for DLQ inspection
- [ ] Correlation ID preserved in DLQ records

### Performance
- [ ] KV metric updates complete in < 50ms
- [ ] No blocking on metric failures
- [ ] Concurrent metric updates handled
- [ ] Promise.all for parallel KV operations

### API Endpoint
- [ ] GET /metrics returns all current metrics
- [ ] Response includes timestamp
- [ ] Response format matches Metrics interface
- [ ] Error handling returns 500 on KV failure

### Metrics Retrieval
- [ ] getAllMetrics() returns complete Metrics object
- [ ] All counters queryable and accurate
- [ ] Processing rate calculated from total and timestamp
- [ ] null handling for optional timestamps

### Testing
- [ ] Unit tests for MetricsManager methods
- [ ] Integration test: event → storage → metrics
- [ ] Load test: 100 events → accurate totals
- [ ] Error case: KV failure doesn't break workflow

---

## Dependencies & Context

**From:** docs/PRD.md (Epic 2 section - Metrics Collection FR-4.2)
**Architecture:** docs/architecture.md (KV Storage Structure section)
**Depends On:** Epic 1.1 (Project Setup), Epic 2.3 (Workflow), Epic 2.4 (Storage)
**Enables:** Epic 2.6 (UI Metrics Display), Epic 4 (Observability)

---

## Dev Notes

- KV has eventual consistency (acceptable for metrics)
- Read-modify-write not truly atomic but acceptable approximation
- Metrics failures are non-blocking (don't fail workflow)
- DLQ is inspected via Cloudflare dashboard in MVP
- Processing rate calculation needs non-zero elapsed time
- Correlation ID enables request tracing through DLQ

---
