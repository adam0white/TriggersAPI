---
title: "Epic 2.2 - Queue Consumer Worker: Consume Batches and Extract Events"
status: "Ready for Development"
epic: "Epic 2: Event Processing & Storage + Metrics Display"
priority: "P0"
acceptance_criteria:
  - "Queue consumer handler receives MessageBatch from Cloudflare Queues"
  - "Batch processing extracts individual event messages with correct structure"
  - "Each message contains event_id, payload, and metadata from original POST request"
  - "Consumer logs batch size and processing attempt number"
  - "Error handling: Log failed messages without blocking batch processing"
  - "Ack successful messages after processing (implicit by queue handler return)"
  - "Nack failed messages to trigger queue retries (throw/reject in handler)"
  - "Consumer respects queue configuration: max_batch_size=100, max_batch_timeout=30s"
  - "Dead Letter Queue routing: Forward messages after max_retries exceeded"
  - "Retry counter incremented on each processing attempt"
  - "Correlation ID maintained through batch processing for log tracing"
  - "Console logs structured JSON format for Tail Worker capture"
  - "Performance: Process 100-event batch in < 5 seconds"
  - "Queue binding verified in local wrangler dev environment"
  - "Integration test: Send event via API, verify message appears in batch"
created_at: "2025-11-10"
modified_at: "2025-11-10"
story_size: "Large"
depends_on: "Epic 1.1 - Project Setup, Epic 2.1 - D1 Schema"
---

## Summary

Implement the queue consumer handler that pulls event batches from Cloudflare Queues and extracts individual event messages for processing. This is the async processing bridge between event ingestion and storage.

## Business Value

Enables asynchronous, reliable event processing. Without this consumer, events accepted by the API remain unprocessed and never reach storage, breaking the entire event flow pipeline.

## Technical Requirements

### Queue Configuration (from wrangler.toml)

Already configured in Epic 1.1:

```toml
[[queues.producers]]
binding = "EVENT_QUEUE"

[[queues.consumers]]
queue = "event-queue"
max_batch_size = 100
max_batch_timeout = 30
max_retries = 3
dead_letter_queue = "event-dlq"
```

**Configuration Meaning:**
- `max_batch_size = 100`: Process up to 100 messages per batch
- `max_batch_timeout = 30`: Wait max 30 seconds before processing partial batch
- `max_retries = 3`: Retry failed messages up to 3 times
- `dead_letter_queue = "event-dlq"`: Forward dead-lettered messages to DLQ

### Queue Consumer Implementation

**File Location:** `src/queue/consumer.ts`

**Consumer Handler Structure:**

```typescript
import { MessageBatch } from '@cloudflare/workers-types';
import { EventPayload } from '../types/events';
import { logger } from '../lib/logger';

export interface QueueMessage {
  event_id: string;
  payload: Record<string, any>;
  metadata?: Record<string, any>;
  timestamp: string;
  correlation_id?: string;
}

export async function processEventBatch(
  batch: MessageBatch<QueueMessage>,
  env: Env,
): Promise<void> {
  const batchSize = batch.messages.length;
  const batchId = crypto.randomUUID();

  logger.info('Queue batch received', {
    correlation_id: batchId,
    batch_id: batchId,
    batch_size: batchSize,
    timestamp: new Date().toISOString(),
  });

  // Process each message in parallel
  const results = await Promise.allSettled(
    batch.messages.map(message => processMessage(message, env, batchId))
  );

  // Track processing results
  let successCount = 0;
  let failureCount = 0;

  results.forEach((result, index) => {
    if (result.status === 'fulfilled') {
      successCount++;
      batch.messages[index].ack();
    } else {
      failureCount++;
      logger.error('Message processing failed', {
        correlation_id: batchId,
        message_index: index,
        error: result.reason?.message || 'Unknown error',
      });
      // Don't ack - this triggers queue retry logic
    }
  });

  logger.info('Queue batch processing completed', {
    correlation_id: batchId,
    batch_id: batchId,
    successful: successCount,
    failed: failureCount,
    timestamp: new Date().toISOString(),
  });

  // If all failed, batch will be retried automatically by queue
  // After max_retries exceeded, it goes to DLQ (handled by Cloudflare)
}

async function processMessage(
  message: Message<QueueMessage>,
  env: Env,
  batchId: string,
): Promise<void> {
  const { event_id, payload, metadata, timestamp, correlation_id } = message.body;
  const correlationId = correlation_id || crypto.randomUUID();

  logger.debug('Processing queue message', {
    correlation_id: correlationId,
    event_id,
    batch_id: batchId,
    timestamp,
  });

  // Validate message structure
  if (!event_id || !payload) {
    throw new Error('Invalid message: missing event_id or payload');
  }

  // Extract retry count from message (added by queue on retries)
  const retryCount = message.retryCount || 0;

  logger.info('Queue message extracted', {
    correlation_id: correlationId,
    event_id,
    retry_attempt: retryCount,
    batch_id: batchId,
  });

  // Trigger workflow for multi-step processing
  // (Workflow implementation in Epic 2.3)
  // For now, log that message was extracted successfully

  return Promise.resolve();
}
```

### Worker Entry Point Integration

**Update:** `src/index.ts`

Add queue consumer handler to ExportedHandler:

```typescript
import { processEventBatch } from './queue/consumer';

export default {
  async fetch(request: Request, env: Env, ctx: ExecutionContext): Promise<Response> {
    // ... existing fetch handler (from Epic 1.2)
  },

  async queue(batch: MessageBatch<unknown>, env: Env): Promise<void> {
    // Queue consumer handler
    await processEventBatch(batch, env);
  },

  async tail(events: TraceItem[], env: Env): Promise<void> {
    // ... tail worker stub (implemented in Epic 4)
  },
};
```

### Message Structure

**Queue Message Format** (from API Worker via EVENT_QUEUE.send):

```typescript
interface QueueMessage {
  event_id: string;           // UUID generated by API Worker
  payload: Record<string, any>;  // Original event payload
  metadata?: Record<string, any>; // Optional event metadata
  timestamp: string;          // ISO-8601 when event was ingested
  correlation_id?: string;    // Trace ID from API request
}
```

**Message Lifecycle:**
1. API Worker validates and queues message with EVENT_QUEUE.send()
2. Queue batches messages (up to 100, within 30s timeout)
3. Queue Consumer receives batch via queue handler
4. Consumer extracts each QueueMessage and processes
5. Consumer calls message.ack() on success
6. Consumer throws/doesn't ack on failure (triggers retry)
7. After 3 failed attempts, message routes to DLQ

### Logging Implementation

**Create:** `src/lib/logger.ts`

```typescript
export const logger = {
  debug(message: string, context?: Record<string, any>) {
    console.log(JSON.stringify({
      level: 'debug',
      message,
      timestamp: new Date().toISOString(),
      ...context,
    }));
  },

  info(message: string, context?: Record<string, any>) {
    console.log(JSON.stringify({
      level: 'info',
      message,
      timestamp: new Date().toISOString(),
      ...context,
    }));
  },

  warn(message: string, context?: Record<string, any>) {
    console.warn(JSON.stringify({
      level: 'warn',
      message,
      timestamp: new Date().toISOString(),
      ...context,
    }));
  },

  error(message: string, context?: Record<string, any>) {
    console.error(JSON.stringify({
      level: 'error',
      message,
      timestamp: new Date().toISOString(),
      ...context,
    }));
  },
};
```

### Retry and Dead Letter Queue Handling

**From architecture.md - Failure Handling:**

- **Automatic Retries:** Queue configured with `max_retries = 3`
- **Exponential Backoff:** Cloudflare Queues handles backoff automatically
- **Dead Letter Queue:** Failed messages after 3 retries route to `event-dlq`
- **Consumer Strategy:** Don't ack failed messages → Cloudflare retries

**Implementation Pattern:**

```typescript
// Success: message auto-acked
await processMessage(message, env, batchId);  // Doesn't throw
message.ack(); // Explicit ack

// Failure: message not acked → triggers queue retry
throw new Error('Processing failed');  // Throws → no ack → queue retries
```

### Error Handling Strategy

**Non-Blocking Failures:**

```typescript
// Use Promise.allSettled to process all messages even if some fail
const results = await Promise.allSettled(
  batch.messages.map(msg => processMessage(msg, env, batchId))
);

// Ack successful, track failures
results.forEach((result, index) => {
  if (result.status === 'fulfilled') {
    batch.messages[index].ack();
  }
  // If rejected, don't ack → queue handles retry
});
```

**Message Validation:**

```typescript
function validateMessage(body: unknown): QueueMessage {
  const msg = body as any;

  if (!msg.event_id || typeof msg.event_id !== 'string') {
    throw new Error('Invalid event_id');
  }

  if (!msg.payload || typeof msg.payload !== 'object') {
    throw new Error('Invalid payload');
  }

  return {
    event_id: msg.event_id,
    payload: msg.payload,
    metadata: msg.metadata,
    timestamp: msg.timestamp || new Date().toISOString(),
    correlation_id: msg.correlation_id,
  };
}
```

### Performance Characteristics

**Target Metrics (from PRD - FR-2.1):**
- Process 100-event batch in < 5 seconds
- Queue to processing start: < 5 seconds (normal conditions)
- End-to-end ingestion → queue → consumer: < 10 seconds

**Batch Processing:**
- Consume up to 100 messages per batch
- Process in parallel using Promise.allSettled
- One batch handler invocation per batch
- Cloudflare automatically creates new batches after timeout or size limit

**Memory Efficiency:**
- Don't load entire batch in memory
- Process messages in parallel for better CPU utilization
- Clear large message payloads after processing

### Testing Verification

**Local Testing Steps:**

1. **Start Wrangler with Queue Support:**
   ```bash
   npx wrangler dev
   # Should show: "Listening on http://localhost:8787"
   # With queue consumer registered
   ```

2. **Queue Message Structure Test:**
   Verify wrangler.toml queue binding is recognized:
   ```
   ✓ Queue binding "EVENT_QUEUE" detected
   ```

3. **Send Test Event via API** (once Epic 1.2 is complete):
   ```bash
   curl -X POST http://localhost:8787/events \
     -H "Authorization: Bearer test-token" \
     -H "Content-Type: application/json" \
     -d '{
       "payload": {"user_id": "123", "action": "test"},
       "metadata": {"source": "curl"}
     }'
   ```

4. **Check Queue Consumer Logs:**
   Verify console output shows:
   ```json
   {"level":"info","message":"Queue batch received","batch_size":1}
   {"level":"info","message":"Queue message extracted","event_id":"..."}
   {"level":"info","message":"Queue batch processing completed","successful":1}
   ```

5. **Batch Size Test:**
   Send 50 events, verify single batch processed
   Send 150 events, verify split into 2 batches

6. **Error Handling Test:**
   Send malformed message (missing payload)
   Verify logged as error, not acked
   Check queue retry mechanism

### Typing and Type Safety

**Update:** `src/types/env.ts`

```typescript
import { D1Database, KVNamespace, Queue } from '@cloudflare/workers-types';

export interface Env {
  DB: D1Database;
  AUTH_KV: KVNamespace;
  EVENT_QUEUE: Queue<unknown>;
  PROCESS_EVENT_WORKFLOW: Workflow; // For Epic 2.3
  Environment: 'development' | 'production';
  LOG_LEVEL: 'debug' | 'info' | 'warn' | 'error';
}
```

---

## Implementation Notes

### What Gets Done

1. Create `src/queue/consumer.ts` with `processEventBatch()` handler function
2. Create `src/lib/logger.ts` with structured JSON logging utilities
3. Create `src/queue/validation.ts` with message validation function
4. Update `src/index.ts` to add queue handler to ExportedHandler
5. Update `src/types/env.ts` with Queue binding type
6. Create test file: `test/queue/consumer.test.ts`
7. Verify wrangler.toml has correct queue configuration
8. Test locally: Send event via API and verify queue consumer processes it
9. Commit: `git add src/queue/ src/lib/logger.ts && git commit -m "feat: queue consumer batch processing"`

### Development Workflow

1. Ensure Event Ingestion API (Epic 1.2) is complete and queuing messages
2. Start: `npx wrangler dev`
3. Send test event: `curl -X POST http://localhost:8787/events ...`
4. Monitor console: Watch for queue batch received/processed logs
5. Test batch size: Send 50+ events, verify batching
6. Test error: Send malformed event, verify retry logic
7. Verify DLQ (manual testing in Cloudflare dashboard post-deployment)

### Key Architecture Decisions

**Parallel Processing:** Use Promise.allSettled for parallel message processing within batch

**Non-Blocking Errors:** Don't let one message failure block entire batch

**Correlation IDs:** Maintain tracing through batch processing for observability

**Automatic Retry:** Let Cloudflare Queues handle retry logic (don't manually retry)

**Structured Logging:** JSON format for easy Tail Worker parsing

---

## Acceptance Criteria Verification Checklist

### Queue Consumer Handler
- [ ] Handler receives MessageBatch with correct type signature
- [ ] processEventBatch() exports properly from queue/consumer.ts
- [ ] Handler registered in src/index.ts ExportedHandler
- [ ] Batch size logged correctly (shows actual count)
- [ ] Each message body extracted with correct structure

### Message Extraction
- [ ] event_id extracted as string
- [ ] payload extracted as JSON object
- [ ] metadata extracted (nullable)
- [ ] timestamp preserved from original ingestion
- [ ] correlation_id maintained for tracing

### Error Handling
- [ ] Failed message not acked (triggers retry)
- [ ] Successful message acked explicitly
- [ ] Promise.allSettled used for parallel processing
- [ ] Error logged with correlation_id
- [ ] Batch doesn't fail entirely if one message fails

### Retry and DLQ
- [ ] message.retryCount accessible on retries
- [ ] Retry count logged for observability
- [ ] Queue configuration respected (max_batch_size, max_batch_timeout)
- [ ] After max_retries exceeded, message routes to DLQ (via Cloudflare)

### Logging
- [ ] Structured JSON format for all logs
- [ ] INFO level: batch received, message extracted, batch completed
- [ ] DEBUG level: detailed processing flow
- [ ] ERROR level: failures with error details
- [ ] correlation_id in all logs

### Performance
- [ ] 100-message batch processes in < 5 seconds
- [ ] No blocking I/O in batch processing loop
- [ ] Parallel processing for message handling
- [ ] Memory usage reasonable for 100-message batch

### Local Development
- [ ] `npx wrangler dev` recognizes queue binding
- [ ] Queue consumer handler registered
- [ ] Console logs appear for batch processing
- [ ] Multiple batches handled correctly

---

## Dependencies & Context

**From:** docs/PRD.md (Epic 2 section - Queue-Based Processing FR-2.1)
**Architecture:** docs/architecture.md (Integration Points - Queue → Workflow)
**Depends On:** Epic 1.1 (Project Setup), Epic 1.2 (Event Ingestion), Epic 2.1 (D1 Schema)
**Enables:** Epic 2.3 (Workflow Implementation), Epic 2.4 (Event Storage)

**Cloudflare Docs:**
- [Queues Configuration](https://developers.cloudflare.com/queues/configuration/)
- [Message Structure](https://developers.cloudflare.com/queues/platform/batches/)
- [Error Handling](https://developers.cloudflare.com/queues/platform/retries/)

---

## Dev Notes

- Queue consumer handler executes in background after API returns response
- MessageBatch type provides access to messages array and batch metadata
- Don't manually retry messages - let Cloudflare Queue handle retry logic
- Structured JSON logging is critical for Tail Worker capture and debugging
- Test with multiple batch sizes to verify batching behavior
- DLQ can be inspected via Cloudflare dashboard post-deployment
- correlation_id enables request tracing across async processing

---

## Performance Testing

### Load Test Scenario

```bash
# Send 100 events rapidly
for i in {1..100}; do
  curl -X POST http://localhost:8787/events \
    -H "Authorization: Bearer test-token" \
    -H "Content-Type: application/json" \
    -d "{\"payload\":{\"id\":$i}}" &
done
wait

# Monitor console output
# Verify: 1 batch of 100 (or 2 batches if processed before timeout)
# Check processing time
```

### Batch Size Verification

```bash
# Send 50 events
# Expect: 1 batch with 50 messages
# Processing should complete quickly (partial batch)

# Send 100 events
# Expect: 1 batch with 100 messages
# Processing should take ~5 seconds

# Send 150 events
# Expect: 2 batches (100 + 50)
# Each processed separately
```

---
