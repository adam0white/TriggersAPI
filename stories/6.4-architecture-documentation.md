---
title: "Story 6.4 - Architecture Documentation: System Diagram, Data Flow, Component Descriptions"
status: "Ready for Development"
epic: "Epic 6: Performance Testing + Final Polish"
priority: "P1"
story_size: "Large"
estimated_hours: 8
created_at: "2025-11-12"
modified_at: "2025-11-12"
---

## Summary

Create comprehensive architecture documentation that visualizes the entire TriggersAPI system through diagrams, data flow models, and detailed component descriptions. Documentation should include high-level system architecture diagram showing all components and their relationships, detailed component architecture describing Workers, D1, KV, Queues, Workflows, and Tail Workers, complete data flow diagrams showing event ingestion, processing, storage, and retrieval, sequence diagrams for key operations, technology stack overview, architectural decisions with trade-offs, scalability architecture, security architecture, observability architecture, edge deployment strategy, API architecture patterns, error handling architecture, integration patterns, performance architecture, and future considerations.

## Business Value

Comprehensive architecture documentation demonstrates technical depth and design maturity. Provides stakeholders with clear visibility into system design, component responsibilities, and data flow patterns. Enables future maintainers and developers to understand design decisions, rationale, and implementation patterns. Architecture diagrams serve as reference material during discussions, onboarding, and code reviews. Documentation establishes credibility for production-readiness and scalability claims. Complete architecture story demonstrates mastery of edge-native systems and Cloudflare platform. Essential for showcasing system as blueprint for Zapier's modernization efforts.

## Technical Context

**From PRD (FR-7.2: Documentation):**
- System MUST include API documentation covering all endpoints
- Documentation MUST include endpoint specs with examples
- Documentation MUST include authentication setup
- Documentation MUST be accessible and clear

**From PRD (NFR-6: Developer Experience):**
- Clear separation of concerns (Workers, utilities, types)
- Monorepo structure with logical separation
- Comprehensive inline documentation
- Code organization with clear naming conventions

**From PRD (NFR-7: Maintainability):**
- Architecture overview for future development
- Design decision documentation
- Technology selection rationale
- Scalability and extensibility patterns

**From Architecture Document:**
- Single Worker deployment at 300+ Cloudflare edge locations
- TypeScript-native implementation with strict mode
- Monorepo structure with feature-based grouping
- Worker-to-Worker communication via RPC (no HTTP)
- D1 for durable event storage, KV for metrics
- Queue for async processing with Dead Letter Queue
- Workflows for guaranteed orchestration
- Tail Workers for comprehensive observability

**Key Dependencies:**
- All previous epics (1-5) completed with full implementation
- All endpoints, workflows, and features functional
- Database schema finalized
- KV metrics structure established
- Tail Worker logging implemented
- UI dashboard complete with all components

## Acceptance Criteria

### 1. High-Level System Architecture Diagram

**Criterion 1a: Overall System Visualization**
- Diagram showing all major components at 10,000 ft view
- Components: Cloudflare Edge, API Worker, Queue, Workflow, D1 Database, KV Store, Tail Worker
- Clear boxes/containers for each component
- Arrows showing data flow direction and communication patterns
- Labels for each arrow indicating data type and direction
- Visual distinction between synchronous and asynchronous flows
- Diagram should fit on single page when printed
- SVG, PNG, or Mermaid format (ASCII art acceptable as fallback)

**Criterion 1b: Global Distribution Context**
- Visual representation of 300+ Cloudflare edge locations
- Arrow from users globally pointing to nearest edge location
- D1 primary and replica locations shown
- KV cache location shown as globally distributed
- Indicates sub-100ms latency from edge to user
- Single database access pattern from distributed workers shown

**Criterion 1c: Communication Patterns**
- API requests entering at edge (Cloudflare Workers)
- Events flowing to Queue component
- Queue triggering Workflow execution
- Workflow writing to D1 database
- Workflow updating KV metadata
- Tail Worker capturing all execution data
- Direct RPC calls between Workers (no HTTP shown)
- Clear legend explaining line styles (sync, async, RPC)

### 2. Component Architecture

**Criterion 2a: API Worker Documentation**
- Component name: "API Worker"
- Responsibilities: HTTP request handling, request validation, authentication, response formatting
- Input ports: HTTP requests from clients on /events, /inbox endpoints
- Output ports: Events sent to Queue, responses returned to clients, logs to Tail Worker
- Performance characteristics: <50ms edge response time target, handles 100+ events/sec
- Scaling approach: Automatic global distribution, zero cold starts
- Key code locations: `src/routes/events.ts`, `src/routes/inbox.ts`, `src/middleware/auth.ts`
- API endpoints handled: POST /events, GET /inbox, POST /inbox/:id/ack, POST /inbox/:id/retry, GET /
- Auth implementation: Bearer token validation via KV lookup
- Error handling: Structured error responses with correlation IDs
- Integration points: Validates with Validation module, sends to Queue, queries D1 via Workflow

**Criterion 2b: Queue Consumer Documentation**
- Component name: "Queue Consumer Worker"
- Responsibilities: Consume batches from queue, parse events, trigger workflows, handle retries
- Input: Batched messages from Cloudflare Queues (configurable batch size, default 100)
- Output: Workflow triggers, logs to Tail Worker, failed messages to Dead Letter Queue
- Batch processing: Configurable batch size up to 1000 events, processing timeout 30 seconds
- Retry logic: Automatic retry 3 times with exponential backoff (built-in to Queue)
- Dead Letter Queue: Failed events after max retries routed to DLQ
- Key code locations: `src/queue/consumer.ts`
- Scaling: Processes batches in parallel across edge locations
- Error recovery: Logs failures, routes to DLQ, continues processing
- Integration: Consumes from Events Queue, triggers Process Event Workflow for each event

**Criterion 2c: Workflow Orchestration Documentation**
- Component name: "Process Event Workflow"
- Responsibilities: Guaranteed execution of multi-step event processing (validate → store → metrics)
- Input: Event data from Queue consumer
- Output: Events stored in D1, metrics updated in KV, logs to Tail Worker
- Workflow steps:
  1. Validate event structure (check payload and metadata presence)
  2. Write event to D1 database (events table)
  3. Update KV metrics (increment counters)
  4. Publish success notification
- Guarantees: At-least-once execution, automatic retries on failure, durable state persistence
- Timeout handling: 30-second execution deadline per event
- Key code location: `src/workflows/process-event.ts`
- Workflow configuration: Retry policy, timeout settings in wrangler.toml
- State management: Durable Objects if needed for complex state (current design doesn't require)
- Integration: Receives events from Queue Consumer, writes to D1, updates KV

**Criterion 2d: D1 Database Component**
- Component name: "D1 Database (SQLite)"
- Responsibilities: Durable event storage, query capability, data integrity
- Primary table: `events` with columns: event_id, payload, metadata, status, created_at, updated_at, retry_count
- Data integrity: Primary key on event_id, status checks (pending|delivered|failed)
- Indexes: idx_events_status, idx_events_created_at, idx_events_status_created for query optimization
- Replication: Automatic managed replication for read replicas at edge locations
- Storage size: Supports 500MB+ databases on paid plans
- Query patterns: Inbox queries by status and timestamp, fast lookups by event_id
- Key code location: `src/db/schema.sql`, `src/db/queries.ts`
- Performance: <50ms query response time for inbox lookups (with indexes)
- Backup/Recovery: Cloudflare managed, automatic replication
- Integration: Written to by Workflow, queried by API Worker (inbox endpoints)

**Criterion 2e: KV Store Component**
- Component name: "Cloudflare KV Store"
- Responsibilities: Fast metadata storage, real-time metrics, auth token validation
- Data categories:
  1. Auth tokens: `auth:token:<token>` with validation metadata
  2. Metrics: `metrics:events:total`, `metrics:events:pending`, etc. with counters
  3. Queue state: `queue:depth` for approximate queue depth
  4. DLQ state: `dlq:count` for dead letter queue size
  5. Custom profiles: `profile:<id>` for saved configurations
- Consistency model: Eventual consistency (acceptable for metrics), strong for auth
- Atomic operations: Used for incrementing metrics safely
- TTL: Auth tokens with 24-hour TTL, metrics with no expiration
- Performance: <1ms lookups, sub-100ms writes
- Key code location: `src/lib/metrics.ts`
- Integration: Auth middleware reads tokens, metrics collection writes counters, UI reads for display

**Criterion 2f: Tail Worker Component**
- Component name: "Tail Worker (Observability)"
- Responsibilities: Capture all Worker executions, collect logs, track metrics, store for dashboard
- Input: All Worker execution data automatically captured by Cloudflare
- Captures:
  1. Request/response data (headers, timing, status codes)
  2. Console logs from all Workers
  3. Exceptions and errors
  4. Performance timing (CPU time, wall-clock time)
  5. Latency metrics per request
- Storage: Logs stored in D1 (`worker_logs` table) or KV for real-time dashboard access
- Log structure: Timestamp, level (debug|info|warn|error), message, correlation_id, context
- Real-time delivery: Logs available to dashboard within 1-2 seconds
- Key code location: `src/tail/worker.ts`
- Configuration: Tail Workers enabled in wrangler.toml
- Retention: Default 7 days, configurable
- Performance impact: Minimal (<1% overhead on total execution time)
- Integration: Captures from all Workers, provides logs to dashboard

**Criterion 2g: API Routes Breakdown**
- POST /events: Handled by API Worker, validates payload, sends to Queue, returns event_id
- GET /inbox: Handled by API Worker, queries D1 with filters, returns event list with pagination
- POST /inbox/:id/ack: Handled by API Worker, deletes from D1, updates KV metrics
- POST /inbox/:id/retry: Handled by API Worker, requeues event, increments retry_count
- GET /: Served by API Worker, returns dashboard HTML/CSS/JS
- Internal endpoints: Debug endpoints for performance testing, metrics endpoints

### 3. Data Flow Diagrams

**Criterion 3a: Event Ingestion Flow**
- Flow diagram showing step-by-step event ingestion process
- User/System → POST /events with {payload, metadata}
- API Worker receives request at edge location
- Validation step: Checks payload structure, payload required, metadata optional
- Auth step: Validates Bearer token from KV
- Event ID generation: Creates UUID v4
- Queue send: Sends event to Cloudflare Queue
- Response: Returns 200 with event_id, status "accepted", timestamp
- Diagram shows timings: <50ms total for ingestion
- Error paths: 400 for bad payload, 401 for auth failure, 503 if queue full
- Diagram includes: Request → Validate → Auth → Queue → Response flow

**Criterion 3b: Event Processing & Storage Flow**
- Flow diagram showing Queue → Workflow → D1 pipeline
- Queue Consumer receives batch of events (configurable size)
- Parse batch: Extract individual events from batch
- For each event in batch:
  1. Trigger Process Event Workflow
  2. Workflow validates event structure
  3. Write to D1 events table (status: pending)
  4. Update KV metrics (increment total, pending counts)
  5. Mark as processed
- On failure: Route to Dead Letter Queue, update error metrics
- Timing: <10 seconds end-to-end (queue → storage)
- Diagram shows: Batch → Parse → Validate → Write → Metrics → Success/DLQ
- Includes error path with retry logic

**Criterion 3c: Event Retrieval Flow**
- Flow diagram showing inbox query process
- User: GET /inbox?status=pending&from=...&to=...
- API Worker receives at edge
- Auth validation via KV token lookup
- Build query: SQL WHERE clause based on filters
- Database query: SELECT from events table with indexes
- Format results: Transform DB rows to API response format
- Pagination: Apply limit and offset
- Response: Return events array, total count, pagination metadata
- Timing: <200ms at p95 (including D1 query)
- Diagram shows: Request → Auth → Query Build → DB Query → Format → Response

**Criterion 3d: Event Acknowledgment Flow**
- Flow diagram showing deletion and cleanup process
- User: POST /inbox/:id/ack
- API Worker validates event_id format
- Auth check via KV
- Workflow triggers: Delete from D1, update KV metrics
- Success: Event removed, event count decremented
- Response: 200 with confirmation
- Timing: <150ms at p95
- Diagram shows: Request → Auth → Delete → Metrics Update → Response

**Criterion 3e: Observability Data Collection Flow**
- Flow diagram showing metrics collection and dashboard feed
- All Workers execution → Tail Worker (automatic capture)
- Tail Worker processes: Logs, timing data, error tracking
- Storage: Stores in D1 worker_logs table with timestamp
- Dashboard polling: GET /metrics endpoint reads from KV (for real-time) and D1 (for historical)
- Real-time metrics: Incremental counters in KV updated by Workflow
- Dashboard display: Real-time updates every 2-5 seconds
- Diagram shows: Workers → Tail → Storage → KV Metrics → Dashboard

**Criterion 3f: Debug Flow**
- Flow diagram showing debug flag triggering error pathways
- User: POST /events?debug=validation_error
- API Worker checks debug parameter
- Branch: If debug flag present, trigger error simulation
- validation_error: Return 400 with sample validation error
- processing_error: Queue event, then return 500 error
- queue_delay: Inject 2-second delay before response
- dlq_routing: Event sent to DLQ after queue processing
- Response: Returns appropriate error or delayed response
- Diagram shows decision tree with different paths per debug flag

### 4. Sequence Diagrams for Key Operations

**Criterion 4a: Successful Event Ingestion Sequence**
- Title: "Happy Path - Event Ingestion"
- Participants: User, API Worker, Queue, Workflow, D1, KV, Tail Worker
- Message sequence:
  1. User → API: POST /events with payload
  2. API → Auth Middleware: Check Bearer token
  3. Auth → KV: Validate token (KV lookup)
  4. KV → Auth: Token valid response
  5. API → Validation: Validate payload structure
  6. Validation → API: Valid response
  7. API → UUID: Generate event ID
  8. API → Queue: Send event message
  9. Queue → API: Acknowledgement (message queued)
  10. API → User: 200 response with event_id
  11. Queue → Workflow: Trigger (async)
  12. Workflow → D1: Insert event record
  13. D1 → Workflow: Insert success
  14. Workflow → KV: Update metrics (increment counters)
  15. KV → Workflow: Update success
  16. Workflow → Tail: Log event processed
  17. Tail → D1: Store log record
- Total time client sees response: <50ms
- Total time event fully processed: <10 seconds

**Criterion 4b: Event Processing with Retry Sequence**
- Title: "Retry Path - Failed Event Processing"
- Participants: Queue, Workflow, D1, Dead Letter Queue, KV
- Scenario: Database write fails, triggering retry
- Message sequence:
  1. Queue → Workflow: Deliver batch message
  2. Workflow → D1: Attempt insert (fails - connection error)
  3. D1 → Workflow: Error response
  4. Workflow → Workflow: Log failure, increment retry counter
  5. Workflow: Wait exponential backoff (2^attempt seconds)
  6. Workflow → D1: Retry insert (succeeds on retry)
  7. D1 → Workflow: Success response
  8. Workflow → KV: Update metrics
  9. Queue removes message from retry queue
- If all retries fail (3 max):
  1. Queue → Dead Letter Queue: Move message to DLQ
  2. DLQ → KV: Update DLQ counter
  3. Alert system notified (future feature)

**Criterion 4c: Inbox Query Sequence**
- Title: "Inbox Retrieval - Filter and Paginate"
- Participants: User, API Worker, Auth, D1, User Interface
- Message sequence:
  1. User → API: GET /inbox?status=pending&limit=50&offset=0
  2. API → Auth: Check Bearer token
  3. Auth → KV: Token validation
  4. KV → Auth: Valid response
  5. API → Query Builder: Build SQL from filters
  6. Query Builder → D1: SELECT events WHERE status='pending' ORDER BY created_at DESC LIMIT 50 OFFSET 0
  7. D1 → API: Return result set (50 events) + total count
  8. API → Formatter: Format events to API response
  9. Formatter → API: Formatted response ready
  10. API → User: 200 response with events array
  11. User → UI: Display events (pagination buttons shown)
- Timing: <200ms total (D1 query optimized with indexes)

**Criterion 4d: Event Acknowledgment Sequence**
- Title: "Cleanup - Acknowledge and Delete"
- Participants: User, API Worker, Auth, D1, KV
- Message sequence:
  1. User → API: POST /inbox/:id/ack
  2. API → Auth: Validate token
  3. Auth → KV: Check token
  4. KV → Auth: Valid
  5. API → Workflow: Trigger delete workflow
  6. Workflow → D1: DELETE FROM events WHERE event_id = :id
  7. D1 → Workflow: 1 row deleted
  8. Workflow → KV: Decrement metrics counters
  9. KV → Workflow: Update success
  10. Workflow → API: Success response
  11. API → User: 200 with event_id and status "deleted"
- Timing: <150ms at p95

**Criterion 4e: Error Path - Validation Failure**
- Title: "Error Path - Invalid Payload"
- Participants: User, API Worker, Validation
- Scenario: User sends invalid JSON
- Message sequence:
  1. User → API: POST /events with malformed JSON
  2. API → JSON Parser: Parse request body (fails)
  3. Parser → API: SyntaxError
  4. API → Error Handler: Catch and format error
  5. Error Handler → API: Structured error response
  6. API → Tail: Log validation error
  7. Tail → D1: Store error log
  8. API → KV: Increment error counter
  9. API → User: 400 with structured error
- Response includes: error code, message, correlation_id, timestamp
- No queue message created (validation failed before queueing)

### 5. Technology Stack Overview

**Criterion 5a: Runtime Environment**
- Platform: Cloudflare Workers (JavaScript/TypeScript runtime)
- Execution model: Event-driven, request-based
- Memory: 128MB per Worker instance
- CPU time: 50ms per request (paid plan)
- Cold start: Zero (Workers always ready)
- Deployment model: Distributed to 300+ edge locations globally

**Criterion 5b: Core Services Stack**
- Workers: HTTP server runtime (`src/index.ts`, `src/routes/`)
- Queue: Message queue for async processing (Cloudflare Queues)
- Workflows: Durable orchestration engine (Cloudflare Workflows)
- D1: SQLite database at edge (Cloudflare D1)
- KV: Distributed key-value store (Cloudflare KV)
- Tail Workers: Observability system (automatic capture)
- RPC: Worker-to-Worker communication (typed method calls)

**Criterion 5c: Language & Frameworks**
- Language: TypeScript 5.x (strict mode required)
- Runtime: Node.js compatible (via Cloudflare Workers runtime)
- UI Framework: React with shadcn components
- Styling: Tailwind CSS 3.x (utility-first)
- Build tool: Wrangler CLI (official Cloudflare tool)
- Package manager: npm or pnpm

**Criterion 5d: Development Tools**
- Wrangler: Cloudflare CLI for local dev and deployment
- Vitest: Lightweight testing framework (if used)
- ESLint: Code quality and style checking
- Prettier: Code formatting
- TypeScript compiler: Type checking and compilation
- Local development: `wrangler dev` with local emulation

**Criterion 5e: Dependencies**
- External dependencies minimized to leverage Cloudflare platform
- Key npm packages: `hono` (optional HTTP framework), `zod` (schema validation)
- Cloudflare bindings: D1Database, KVNamespace, Queue, Workflow
- React libraries: `react`, `react-dom`, shadcn components

**Criterion 5f: Infrastructure & Deployment**
- Hosting: Cloudflare global edge network (300+ locations)
- Database: D1 with automatic replicas
- Cache: KV global cache, eventual consistency
- Message Queue: Cloudflare Queues with at-least-once delivery
- CI/CD: Via wrangler deploy (can integrate with GitHub Actions)
- Secrets: Environment variables in Cloudflare dashboard

### 6. Design Decisions & Trade-offs

**Criterion 6a: Single Worker vs Microservices**
- Decision: Single Worker deployment with function-based separation
- Rationale: Simpler deployment, zero inter-service latency, easier debugging, sufficient scale for use case
- Trade-off: Less isolation between components, but acceptable for this domain
- Alternative considered: Microservices (separate Workers for API, Queue, Tail)
- Why rejected: Adds complexity, requires inter-Worker HTTP calls, operational overhead

**Criterion 6b: D1 (SQLite) vs External Database**
- Decision: Use Cloudflare D1 (managed SQLite)
- Rationale: Edge-native, automatic replication, zero configuration, excellent for read-heavy workloads
- Trade-off: SQLite limitations (no complex joins at massive scale), but sufficient for event table
- Alternative: External PostgreSQL, DynamoDB
- Why D1 chosen: Aligned with edge-native architecture, zero infrastructure management, built-in replication

**Criterion 6c: Bearer Tokens via KV vs OAuth**
- Decision: Simple Bearer tokens validated against KV store
- Rationale: Fast validation (<1ms), no external auth service, sufficient for MVP showcase
- Trade-off: No OAuth flow, stateless token validation, tokens pre-configured
- Alternative: Full OAuth2, JWT with signature verification
- Why Bearer tokens chosen: Minimal implementation, demo-friendly, fast validation in KV

**Criterion 6d: Workflows vs Queue Consumer**
- Decision: Use Workflows for guaranteed multi-step orchestration
- Rationale: Durable execution guarantees, automatic retries, transactional semantics
- Trade-off: Slightly more complex than simple queue handler
- Alternative: Simple queue consumer with manual retry logic
- Why Workflows: Built-in reliability, Cloudflare best practice, matches guaranteed delivery promise

**Criterion 6e: RPC vs HTTP for Worker Communication**
- Decision: Worker-to-Worker communication via RPC (no HTTP)
- Rationale: Lower latency, typed method calls, no serialization overhead
- Trade-off: Less flexible than HTTP (must use Cloudflare RPC), tighter coupling
- Alternative: Internal HTTP APIs between Workers
- Why RPC chosen: Better performance, type-safe, Cloudflare native

**Criterion 6f: KV vs D1 for Metrics**
- Decision: Real-time metrics in KV, historical metrics in D1
- Rationale: KV for <1ms lookups (dashboard), D1 for queries and trends
- Trade-off: Dual storage, eventual consistency in KV
- Alternative: All metrics in D1 (slower for real-time)
- Why dual storage: Optimizes both real-time display and historical analysis

### 7. Scalability Architecture

**Criterion 7a: Horizontal Scalability**
- Workers: Automatically scaled across 300+ edge locations globally
- No server provisioning: Cloudflare manages auto-scaling
- Request distribution: Anycast routing sends requests to nearest edge location
- Concurrent requests: Handled by multiple Worker instances at each edge location

**Criterion 7b: Queue Scalability**
- Batch processing: Configurable batch size (default 100, max 1000 events)
- Parallel batches: Multiple batches processed simultaneously across locations
- Back pressure handling: Queue automatically manages throughput
- Dead Letter Queue: Failed events don't block subsequent processing

**Criterion 7c: Database Scalability**
- SQLite limitations: Suitable for <10GB databases, current design well under
- Read replicas: D1 automatic replicas at edge for distributed reads
- Write patterns: Single writer (centralized), read replicas at edge
- Index optimization: Status and timestamp indexes for fast inbox queries

**Criterion 7d: Performance Under Load**
- Target: 100+ events/second throughput
- Latency: Sub-100ms at p95 (ingestion), <200ms at p95 (inbox queries)
- Queue depth: Configurable, auto-scales with demand
- Metrics: Real-time metrics tracked in KV for monitoring

**Criterion 7e: Global Distribution Benefits**
- Sub-100ms latency: Events processed at nearest edge location
- Distributed cache: KV replicated globally for fast token lookups
- Edge compute: No central point of failure, distributed processing
- CDN: UI assets cached at edge for fast delivery

### 8. Security Architecture

**Criterion 8a: Authentication**
- Bearer tokens: Required for all API endpoints (except root dashboard)
- Token validation: Fast KV lookup, <1ms latency
- HTTPS enforcement: All communication via TLS 1.2+ (Cloudflare enforced)
- Token storage: Stored in KV with secure configuration

**Criterion 8b: Authorization**
- Access control: All authenticated users have same permissions (MVP)
- Token scope: Single scope for all operations (MVP simplification)
- Future: Per-endpoint permissions, role-based access control

**Criterion 8c: Input Validation**
- Payload validation: Checks structure before queueing
- JSON parsing: Safe parsing with error handling
- Payload size limits: Max 1MB per event
- SQL injection: Parameterized queries via D1

**Criterion 8d: Data Protection**
- Encryption at rest: D1 encryption handled by Cloudflare
- Encryption in transit: TLS 1.2+ for all connections
- PII handling: No sensitive data in mock demo
- Token exposure: Never logged or exposed in responses

**Criterion 8e: Logging & Audit**
- Structured logging: All requests logged via Tail Worker
- Log retention: 7 days minimum, configurable
- Sensitive data filtering: Tokens and credentials never logged
- Error tracking: All failures tracked with correlation IDs

### 9. Observability Architecture

**Criterion 9a: Metrics Collection**
- Real-time metrics: Event counts by status, queue depth, error rates, latency percentiles
- Storage: KV for real-time (fast access), D1 for historical (trend analysis)
- Update frequency: Metrics incremented on each operation, dashboard refreshes every 2-5 seconds
- Metric types: Counters (totals), gauges (queue depth), histograms (latencies)

**Criterion 9b: Logging**
- Tail Worker: Automatic capture of all Worker executions
- Log types: Request/response data, console logs, errors, timing metrics
- Log storage: D1 worker_logs table with timestamp, level, message, correlation_id
- Log streaming: Dashboard can stream logs in real-time

**Criterion 9c: Distributed Tracing**
- Correlation IDs: UUID generated per request, propagated through all operations
- Trace visibility: Correlation ID in logs, API responses, error messages
- End-to-end tracing: Follow single event from ingestion through storage
- Future: OpenTelemetry integration for detailed distributed tracing

**Criterion 9d: Performance Monitoring**
- Latency measurement: performance.now() for precise timing
- Percentile calculation: p50, p95, p99 for latency distribution
- Throughput tracking: Events processed per second
- Error rate: Failed events as percentage of total

**Criterion 9e: Alerting (Future)**
- Error rate thresholds: Alert if error rate > 5%
- Queue depth: Alert if queue depth > 10,000
- Dead Letter Queue: Alert if DLQ receives messages
- Latency SLA: Alert if p95 latency > 100ms

### 10. Edge Deployment Architecture

**Criterion 10a: Global Distribution**
- 300+ edge locations: Workers deployed to all Cloudflare data centers
- Anycast routing: Users routed to nearest edge location automatically
- Sub-100ms latency: Due to geographic distribution
- Local processing: Events processed at edge, minimal backhaul

**Criterion 10b: Edge Compute Model**
- Cloudflare Workers: Isolated JavaScript runtime per request
- Instant startup: Zero cold start time (Workers always ready)
- Resource limits: 50ms CPU time, 128MB memory per request
- Cost model: Pay per request (serverless pricing)

**Criterion 10c: Database at Edge**
- D1 edge replicas: Read replicas deployed near major edge locations
- Replication lag: Sub-second for eventual consistency
- Write semantics: Single writer, distributed reads
- Cache layer: KV used for frequently accessed data

**Criterion 10d: Failover & Resilience**
- Automatic failover: If one edge location fails, requests routed to next
- Database replication: D1 automatically replicates across multiple data centers
- Queue resilience: Messages persisted until processed
- No single point of failure: All components distributed

### 11. API Architecture Patterns

**Criterion 11a: RESTful Design**
- HTTP methods: POST for mutations (create), GET for queries, proper semantics
- Resource-based URLs: /events, /inbox endpoints represent resources
- Status codes: Standard codes (200, 400, 401, 404, 409, 500, 503)
- Stateless: Each request complete, no session state required

**Criterion 11b: Request/Response Format**
- Content-Type: application/json for all payloads
- Structured errors: error.code, error.message, error.timestamp, error.correlation_id
- Pagination: limit, offset, total for large result sets
- Timestamps: ISO-8601 format for all dates

**Criterion 11c: Authentication Pattern**
- Bearer token header: Authorization: Bearer <token>
- Per-request validation: Token checked on every API call
- Fast validation: KV lookup < 1ms
- Error response: 401 for invalid/missing tokens

**Criterion 11d: Query Patterns**
- Filter parameters: status, from, to for inbox queries
- Pagination: limit and offset for controlling result set
- Sorting: Implicit by created_at descending for inbox
- Result formatting: Consistent field names and structure

### 12. Error Handling Architecture

**Criterion 12a: Error Response Format**
- Standardized structure: Always returns error object with code, message, timestamp, correlation_id
- Machine-readable codes: INVALID_PAYLOAD, UNAUTHORIZED, NOT_FOUND, CONFLICT, INTERNAL_ERROR
- Human-readable messages: Clear description of what went wrong
- Timestamp: When error occurred (ISO-8601)
- Correlation ID: For tracing and support

**Criterion 12b: Error Categories**
- Client errors (4xx): Validation failures, auth failures, resource not found
- Server errors (5xx): Unexpected failures, unhandled exceptions
- Service errors (503): Queue full, system degraded, temporary unavailable

**Criterion 12c: Debug Flags**
- validation_error: Demonstrates validation failure handling
- processing_error: Demonstrates processing error recovery
- queue_delay: Demonstrates latency handling
- dlq_routing: Demonstrates dead letter queue handling

**Criterion 12d: Error Recovery**
- Automatic retries: Queue automatically retries up to 3 times
- Exponential backoff: 2^attempt second delay between retries
- Dead Letter Queue: Messages routed to DLQ after max retries
- Logging: All failures logged with full context

### 13. Integration Patterns

**Criterion 13a: Queue Integration**
- Producer: API Worker sends events to Queue
- Consumer: Workflow triggered by Queue messages
- Batching: Events grouped into batches for efficiency
- At-least-once delivery: Guaranteed delivery via Queue implementation

**Criterion 13b: Workflow Integration**
- Trigger: Workflow initiated when queue consumer processes batch
- Steps: Validate → D1 write → KV update (multi-step orchestration)
- Error handling: Automatic retries, failed events to DLQ
- State: Durable state managed by Workflow runtime

**Criterion 13c: Database Integration**
- Query builder: Converts API filters to SQL
- Parameterized queries: Prevent SQL injection
- Indexes: Used for fast lookups (status, created_at)
- Transactions: Implicit via SQL, explicit for multi-step operations

**Criterion 13d: KV Integration**
- Token validation: Auth middleware reads tokens from KV
- Metrics updates: Workflows increment counters atomically
- Caching: Frequently accessed data (auth tokens, metrics) in KV
- Atomic operations: Used for safe counter increments

### 14. Performance Architecture

**Criterion 14a: Request Path Optimization**
- API Worker: <50ms edge response time target
- Queue: <5 second latency from ingestion to processing
- Workflow: <10 seconds end-to-end execution
- Database: <50ms query time (with indexes)

**Criterion 14b: Network Optimization**
- Edge processing: Events handled at nearest edge location
- Minimal backhaul: D1 accessed via local replicas
- CDN caching: UI assets cached at edge
- Compression: Gzip for responses

**Criterion 14c: Resource Efficiency**
- Memory: 128MB per Worker instance
- CPU time: 50ms per request budget
- Batch processing: 100-1000 events per batch
- Caching: KV for frequently accessed data

**Criterion 14d: Scaling Characteristics**
- Horizontal: Auto-scales to handle traffic spikes
- Vertical: Single Worker optimized for minimal resource usage
- Batch efficiency: Processing 1000 events uses minimal overhead vs 100
- Latency consistency: Edge latency doesn't increase with load

### 15. Future Architecture Considerations

**Criterion 15a: Multi-Tenant Architecture**
- Workspace isolation: Each tenant has isolated data
- Auth enhancement: API key per workspace, permission model
- Data separation: Query filters by tenant_id
- Billing integration: Track usage per tenant

**Criterion 15b: Advanced Observability**
- Distributed tracing: OpenTelemetry integration
- Custom metrics: Business-level metrics beyond system metrics
- Alerting: Threshold-based alerts with notification channels
- Dashboard: Historical trend analysis and forecasting

**Criterion 15c: Event Transformation & Routing**
- Transformation pipelines: Pre-process events before storage
- Intelligent routing: Route events based on content analysis
- Event tagging: Automatically tag/categorize events
- Replay capability: Reprocess events from historical archive

**Criterion 15d: Advanced Performance Features**
- Rate limiting: Per-token rate limits
- Circuit breaker: Graceful degradation if downstream services fail
- Priority queues: Process critical events first
- Predictive scaling: ML-based scaling predictions

**Criterion 15e: Enterprise Capabilities**
- Encryption at rest: Customer-managed keys
- Compliance: SOC 2, GDPR, HIPAA certifications
- SLA monitoring: Uptime guarantees with credits
- Audit trails: Complete operation audit logs

## Dependencies

- **All previous epics (1-5):** Complete implementation of all features
- **Story 1.2:** API Worker with working endpoints
- **Story 2.1:** D1 database schema and queries
- **Story 2.3:** Workflow orchestration implemented
- **Story 2.5:** KV metrics collection working
- **Story 4.1:** Tail Worker capturing logs
- **Story 5.4:** UI dashboard complete
- **Story 6.2:** API documentation covering endpoints
- **Story 6.3:** Setup documentation for local development

## Technical Specifications

### Diagram Tools & Formats

**Supported Diagram Formats:**
- Mermaid (embedded in markdown, renders in GitHub)
- PlantUML (text-based, renders in various tools)
- SVG (vector format, scalable)
- PNG (raster, traditional)
- ASCII art (fallback for simple diagrams)

**Recommended Tool Choices:**
1. **Mermaid** - Best for GitHub integration, automatically rendered in .md files
2. **PlantUML** - Better for complex diagrams, supports all diagram types
3. **Lucidchart / Draw.io** - GUI tools, exports to SVG/PNG

**Minimum Requirements:**
- All 15 main diagrams in at least one format
- Diagrams fit on standard page when printed (single or 2-page max)
- Readable text labels and legend
- Clear distinction between components

### Document Structure

```
docs/architecture.md

# TriggersAPI - Comprehensive Architecture Documentation

## 1. System Architecture Diagram
[Main diagram showing all components]

## 2. Component Architecture
### API Worker
### Queue Consumer
### Workflow Orchestration
### D1 Database
### KV Store
### Tail Worker
### API Routes

## 3. Data Flow Diagrams
### Event Ingestion Flow
### Event Processing & Storage Flow
### Event Retrieval Flow
### Event Acknowledgment Flow
### Observability Collection Flow
### Debug Flow

## 4. Sequence Diagrams
### Successful Event Ingestion
### Event Processing with Retry
### Inbox Query
### Event Acknowledgment
### Error Path

## 5. Technology Stack Overview

## 6. Design Decisions & Trade-offs

## 7. Scalability Architecture

## 8. Security Architecture

## 9. Observability Architecture

## 10. Edge Deployment Architecture

## 11. API Architecture Patterns

## 12. Error Handling Architecture

## 13. Integration Patterns

## 14. Performance Architecture

## 15. Future Architecture Considerations
```

## Implementation Workflow

### Phase 1: Diagram Creation
1. Create high-level system architecture diagram (all components)
2. Create component diagrams (API Worker, Queue, Workflow, D1, KV, Tail Worker)
3. Create data flow diagrams (5 main flows)
4. Create sequence diagrams (5 key operations)
5. Choose diagram format (Mermaid recommended)

### Phase 2: Documentation Writing
1. Write technology stack section (runtime, services, languages, tools)
2. Document design decisions (6 key decisions with rationale)
3. Write scalability section (horizontal, queue, database, performance)
4. Write security section (auth, authorization, validation, data protection)
5. Document observability (metrics, logging, tracing, monitoring)

### Phase 3: Advanced Architecture
1. Document edge deployment (global distribution, compute model, resilience)
2. Document API patterns (RESTful, request/response, auth, query patterns)
3. Document error handling (response format, categories, recovery)
4. Document integration patterns (queue, workflow, database, KV)
5. Document performance architecture (optimization, scaling, efficiency)

### Phase 4: Future Considerations
1. Document multi-tenant architecture
2. Document advanced observability enhancements
3. Document event transformation capabilities
4. Document advanced performance features
5. Document enterprise capabilities

### Phase 5: Review & Polish
1. Ensure all 15 criteria covered completely
2. Verify diagrams are clear and complete
3. Cross-reference between sections
4. Add linking between related sections
5. Final proof-read and formatting

## Verification Checklist

### Diagrams
- [ ] High-level system architecture diagram shows all major components
- [ ] Global distribution context shown (300+ edge locations)
- [ ] Communication patterns clear (sync, async, RPC)
- [ ] Component architecture diagrams for all 6+ components
- [ ] Data flow diagrams for 6 key processes (ingestion, processing, retrieval, ack, observability, debug)
- [ ] Sequence diagrams for 5 key operations
- [ ] All diagrams are readable and self-explanatory
- [ ] Legends provided for diagram symbols
- [ ] Diagrams fit on printable pages

### Documentation
- [ ] Technology stack section complete (runtime, services, languages, tools)
- [ ] Design decisions documented with rationale and trade-offs
- [ ] 6+ key design decisions clearly explained
- [ ] Scalability architecture documented (horizontal, queue, database, global)
- [ ] Security architecture documented (auth, input validation, data protection)
- [ ] Observability architecture documented (metrics, logging, tracing)
- [ ] Edge deployment architecture explained
- [ ] API architecture patterns documented
- [ ] Error handling architecture explained
- [ ] Integration patterns documented
- [ ] Performance architecture documented
- [ ] Future considerations outlined (multi-tenant, observability, transformation, etc.)

### Completeness
- [ ] All 15 acceptance criteria addressed
- [ ] Each criterion thoroughly explained with details
- [ ] References to code locations (src/routes/, src/workflows/, etc.)
- [ ] Performance characteristics documented (latency, throughput targets)
- [ ] Security considerations explained
- [ ] Scalability approach described
- [ ] Failure modes and error handling covered
- [ ] Future extensibility discussed

### Quality
- [ ] Diagrams are professional quality
- [ ] Documentation is clear and concise
- [ ] Technical accuracy verified
- [ ] Consistent terminology used throughout
- [ ] Cross-references between sections
- [ ] No outdated references
- [ ] All links working (if markdown links used)

## Notes

- Architecture documentation complements API docs (Story 6.2) and setup docs (Story 6.3)
- Diagrams should be visual learning aids - support not replace text
- Include performance targets (SLOs) in relevant sections
- Reference PRD requirements to show completeness
- Consider audience: technical stakeholders, future developers, system maintainers
- Diagrams can evolve as system grows - document as current state
- Include rationale for decisions (helps future maintainers understand "why")
- Security section critical for compliance and stakeholder confidence
- Performance architecture section demonstrates engineering maturity

## Related Stories

- **6.2:** API Documentation - Endpoint specs with examples
- **6.3:** Setup Documentation - Local dev guide, deployment steps
- **6.1:** Performance Testing - Load testing and metrics
- **All previous epics:** Implementation details referenced in architecture
- **PRD:** Requirements and scope that shaped architecture
- **Architecture.md (existing):** Base architecture to enhance

---

## Story Status

**Created**: 2025-11-12
**Status**: Ready for Development
**Assigned to**: @dev
**Dependencies**: All Stories 1-6.3 completed

Architecture documentation story ready for dev implementation. Detailed acceptance criteria covering all 15 architectural aspects. Should result in comprehensive architecture documentation (25-35 pages) with 10+ diagrams.

---

*Story 6.4: Comprehensive architecture documentation to demonstrate system design maturity, technical depth, and production-readiness.*
