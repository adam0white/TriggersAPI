---
title: "Story 4.1 - Tail Worker Setup: Capture All Worker Executions"
status: "Ready for Development"
epic: "Epic 4: Observability & Tail Worker Logs Display"
priority: "P0"
story_size: "Large"
estimated_hours: 6
created_at: "2025-11-11"
modified_at: "2025-11-11"
---

## Summary

Implement Cloudflare Tail Worker infrastructure to capture all Worker executions, logs, and metrics across the entire system. This worker will collect execution data from API Workers, Queue Consumers, and Workflows, storing it for later retrieval by the dashboard.

## Business Value

Provides complete observability into system behavior, enabling live monitoring, debugging, and performance analysis. Tail Workers are essential for demonstrating that the system is working correctly and tracking all execution paths.

## Technical Context

**From PRD (FR-4.1: Tail Worker Logging):**
- System MUST implement Tail Worker for observability
- Tail Worker MUST capture:
  - Request/response data for all API calls
  - Console logs from all Workers
  - Uncaught exceptions and errors
  - Execution timing and latency metrics
- Tail Worker MUST store logs in accessible format (KV or D1)

**From Architecture:**
- Tail Workers capture automatic observability
- Logs stored in KV or D1 for dashboard consumption
- Structured JSON logging format for all workers
- Correlation IDs for request tracing

## Acceptance Criteria

1. **Tail Worker Initialization**
   - Cloudflare Tail Worker configured in wrangler.toml with correct binding
   - Tail Worker entry point created at `src/tail/worker.ts`
   - Tail Worker imports and uses Tail event types from @cloudflare/workers-types
   - Exports TailWorkerEntrypoint for Worker binding

2. **Request/Response Capture**
   - Tail Worker receives fetch events (incoming requests)
   - Extracts request method, URL path, status code, headers
   - Captures response time (executions[].cpu_milliseconds)
   - Records timestamp in ISO-8601 format for each request
   - Handles both API requests and internal RPC calls

3. **Console Log Capture**
   - Tail Worker receives console log events
   - Extracts log level (debug, info, warn, error)
   - Preserves log message and any arguments
   - Associates logs with correlation_id from context
   - Maintains log ordering within requests

4. **Exception & Error Capture**
   - Tail Worker receives exception events (uncaught errors)
   - Extracts error message, stack trace, and error type
   - Captures exception timestamp and Worker context
   - Marks exceptions with "error" log level
   - Preserves correlation_id for error tracing

5. **Execution Timing Metrics**
   - Captures CPU milliseconds from execution profile
   - Records wall-clock time for request processing
   - Calculates latency percentiles data point (timing data collected)
   - Stores timing in milliseconds with precision to 1ms
   - Handles both successful and failed executions

6. **Log Storage in D1**
   - Creates `tail_logs` table in D1 with proper schema
   - Columns: log_id (PK), worker_name, request_id, correlation_id, log_level, message, context_json, timestamp, execution_time_ms
   - Batch inserts logs in groups (max 100 logs per batch) for efficiency
   - Handles D1 write failures gracefully (log to console but don't block)
   - Implements log rotation (keep last 7 days of logs)

7. **Structured Logging Format**
   - All logs stored in consistent JSON structure with fields:
     - worker_name: Which Worker generated the log
     - correlation_id: Request tracing ID (UUID)
     - log_level: debug|info|warn|error
     - message: Human-readable log message
     - context: Additional structured data (method, path, status, latency, etc.)
     - timestamp: ISO-8601 timestamp
   - Ensures all components follow this schema for consistency

8. **Worker Name Identification**
   - Logs include worker_name field (api-worker, queue-consumer, etc.)
   - Detects worker context from FetchEvent or script metadata
   - Supports identifying logs from different Worker functions
   - Handles tail logs from nested workers correctly

9. **Correlation ID Tracking**
   - Extracts correlation_id from request headers or context
   - Preserves correlation_id through entire request lifecycle
   - All logs for same request share correlation_id
   - Enables request tracing from ingestion through storage

10. **Batching & Performance**
    - Collects logs in memory buffer (max 100 logs)
    - Flushes batch to D1 every 5 seconds OR when buffer full
    - Uses D1 transactions for batch insert atomicity
    - Handles backpressure gracefully (drops oldest logs if D1 slow)
    - Tail Worker execution time < 100ms per call

11. **Error Handling**
    - Catches and handles D1 write failures without blocking
    - Implements retry logic for transient D1 errors (max 3 retries)
    - Falls back to KV if D1 unavailable (temporary storage)
    - Logs Tail Worker failures to console for debugging
    - Never throws exceptions that could break other Workers

12. **Tail Worker Binding Configuration**
    - wrangler.toml includes tail_consumer configuration:
      ```toml
      [[tail_consumers]]
      service = "triggers-api"
      ```
    - Tail Worker is deployed and active
    - Receives logs from main Worker on every invocation
    - Service binding allows Worker to call itself

13. **Type Safety**
    - TypeScript types for all log structures
    - Tail event types from @cloudflare/workers-types imported
    - Interfaces for TailLog, TailEvent, ExecutionContext
    - Strict type checking for log processing functions
    - Handles any Tail event type correctly

14. **Verification & Testing**
    - Tail Worker receives test POST request and captures it
    - Console logs appear in tail_logs table within 10 seconds
    - Exceptions thrown from API Worker are captured
    - Correlation IDs present in all logs for same request
    - D1 query returns logs in chronological order

15. **Documentation**
    - Code comments explain Tail Worker lifecycle and responsibilities
    - README section documents how logs are captured and stored
    - Architecture.md updated with Tail Worker flow diagram
    - Developer notes on correlation ID usage for debugging

## Dependencies

- **Epic 2 Complete:** D1 schema and KV setup required
- **Architecture:** Tail Worker patterns and log structure defined
- **Existing Workers:** API Worker, Queue Consumer must exist to capture logs from

## Technical Specifications

### D1 Table Schema

```sql
CREATE TABLE tail_logs (
  log_id TEXT PRIMARY KEY,           -- UUID v4
  worker_name TEXT NOT NULL,         -- api-worker, queue-consumer, etc
  request_id TEXT,                   -- Cloudflare request ID
  correlation_id TEXT,               -- Custom UUID for request tracing
  log_level TEXT NOT NULL,           -- debug|info|warn|error
  message TEXT NOT NULL,             -- Log message
  context_json TEXT,                 -- Additional context as JSON
  timestamp TEXT NOT NULL,           -- ISO-8601
  execution_time_ms INTEGER,         -- CPU time in milliseconds
  created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_tail_logs_timestamp ON tail_logs(timestamp DESC);
CREATE INDEX idx_tail_logs_correlation_id ON tail_logs(correlation_id);
CREATE INDEX idx_tail_logs_worker_name ON tail_logs(worker_name);
CREATE INDEX idx_tail_logs_level ON tail_logs(log_level);
CREATE INDEX idx_tail_logs_created ON tail_logs(created_at DESC);
```

### Tail Worker Implementation Template

```typescript
// src/tail/worker.ts
import { TailWorkerEntrypoint } from '@cloudflare/workers-types';

export class TailWorker extends TailWorkerEntrypoint {
  async fetch(request: Request): Promise<Response> {
    // Return 200 OK - Tail Worker is not meant to handle HTTP
    return new Response('OK');
  }

  async tail(events: any[]): Promise<void> {
    try {
      const logs: any[] = [];

      for (const event of events) {
        if (event.Logs) {
          // Process console logs
          for (const log of event.Logs) {
            logs.push({
              log_id: crypto.randomUUID(),
              worker_name: 'api-worker', // detect dynamically
              request_id: event.Outcome.RequestId,
              correlation_id: event.Outcome.Logs[0]?.correlation_id || crypto.randomUUID(),
              log_level: 'info',
              message: log.Message,
              context_json: JSON.stringify(log),
              timestamp: new Date(event.Timestamps.StartTime).toISOString(),
              execution_time_ms: event.Outcome.Cpu,
            });
          }
        }

        if (event.Exceptions?.length) {
          // Process exceptions
          for (const exception of event.Exceptions) {
            logs.push({
              log_id: crypto.randomUUID(),
              worker_name: 'api-worker',
              request_id: event.Outcome.RequestId,
              correlation_id: crypto.randomUUID(),
              log_level: 'error',
              message: exception.Name + ': ' + exception.Message,
              context_json: JSON.stringify({
                stack: exception.Message,
                timestamp: exception.Timestamp,
              }),
              timestamp: new Date(exception.Timestamp).toISOString(),
              execution_time_ms: event.Outcome.Cpu,
            });
          }
        }
      }

      // Batch insert to D1
      if (logs.length > 0 && this.env.DB) {
        const insertStatement = `
          INSERT INTO tail_logs (log_id, worker_name, request_id, correlation_id,
            log_level, message, context_json, timestamp, execution_time_ms)
          VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        `;

        for (const log of logs) {
          await this.env.DB.prepare(insertStatement).bind(
            log.log_id,
            log.worker_name,
            log.request_id,
            log.correlation_id,
            log.log_level,
            log.message,
            log.context_json,
            log.timestamp,
            log.execution_time_ms
          ).run();
        }
      }
    } catch (error) {
      console.error('Tail Worker error:', error);
      // Don't throw - failing to log shouldn't break the system
    }
  }
}

export default new TailWorker();
```

### Correlation ID Implementation

```typescript
// In API Worker route handler
const correlationId = crypto.randomUUID();
const context = {
  correlationId,
  timestamp: new Date().toISOString(),
  method: request.method,
  path: new URL(request.url).pathname,
};

// Log with correlation ID
console.log(JSON.stringify({
  level: 'info',
  message: 'Event received',
  correlation_id: correlationId,
  context: {
    event_id: eventId,
    payload_size: JSON.stringify(payload).length,
  },
}));

// Include in response headers for client tracing
response.headers.set('X-Correlation-ID', correlationId);
```

### wrangler.toml Configuration

```toml
# Tail Worker service binding
[[services]]
binding = "TAIL_WORKER"
service = "tail-worker"

# Or configure as tail consumer for main worker
[[tail_consumers]]
service = "triggers-api"
```

## Implementation Workflow

1. **Add Tail Worker Entry Point**
   - Create `src/tail/worker.ts` implementing TailWorkerEntrypoint
   - Implement `tail()` method to process tail events
   - Add proper TypeScript types and error handling

2. **Create D1 Table**
   - Run migration to create `tail_logs` table
   - Add indexes for timestamp, correlation_id, worker_name, log_level
   - Test schema with sample inserts

3. **Implement Event Processing**
   - Extract request/response from Tail events
   - Capture console logs and exceptions
   - Build structured log objects

4. **Add D1 Batch Insertion**
   - Collect logs in buffer
   - Batch insert every 5 seconds or when buffer full
   - Handle D1 write failures gracefully

5. **Integrate Correlation IDs**
   - Update API Worker to generate and log correlation IDs
   - Preserve correlation IDs through Queue Consumer
   - Include in all console logs

6. **Configure wrangler.toml**
   - Add Tail Worker binding
   - Update service configuration
   - Test deployment

7. **Verify Tail Capture**
   - Send test requests to POST /events
   - Verify logs appear in tail_logs table
   - Check correlation IDs are preserved

## Verification Checklist

- [ ] Tail Worker initializes without errors
- [ ] tail_logs table exists with correct schema
- [ ] Test request captured with correct method/path/status
- [ ] Console logs appear in tail_logs within 10 seconds
- [ ] Exceptions are captured with error log level
- [ ] Correlation IDs present in all logs for same request
- [ ] Execution timing (cpu_milliseconds) recorded accurately
- [ ] Batch insert working (multiple logs per batch)
- [ ] D1 write failures don't break other Workers
- [ ] Logs queryable by timestamp and correlation_id
- [ ] TypeScript compilation succeeds with no errors
- [ ] Tail Worker documented in README

## Notes

- Tail Worker is asynchronous and non-blocking - it observes all requests but doesn't impact performance
- Logs are stored in D1 for persistence; can query for historical analysis
- Correlation IDs enable end-to-end tracing across entire request lifecycle
- Log retention policy set to 7 days; can be adjusted in production
- Watch for D1 write limits - batch insertion optimizes for throughput

## Related Stories

- **4.2:** Log Processing - Parse and enhance captured log data
- **4.3:** Metrics Calculation - Compute latency percentiles and error rates from logs
- **4.4:** UI Logs Display - Stream logs to dashboard
- **4.5:** UI Metrics Enhancement - Visualize metrics from Tail Worker data
