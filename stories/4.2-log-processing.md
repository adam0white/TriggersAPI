---
title: "Story 4.2 - Log Processing: Parse and Store Request/Response/Error Data"
status: "Ready for Development"
epic: "Epic 4: Observability & Tail Worker Logs Display"
priority: "P0"
story_size: "Large"
estimated_hours: 5
created_at: "2025-11-11"
modified_at: "2025-11-11"
---

## Summary

Implement advanced log processing pipeline that parses raw Tail Worker events, extracts request/response details, error information, and timing data, then stores enriched logs in D1 with proper categorization and filtering capabilities.

## Business Value

Transforms raw event data into actionable insights, enabling developers to understand system behavior, debug issues, and trace requests end-to-end. Proper parsing ensures logs are queryable and valuable for observability.

## Technical Context

**From PRD (FR-4.2: Metrics Collection):**
- System MUST track in KV: latency percentiles (p50, p95, p99), error rates
- Metrics MUST update in real-time or near-real-time

**From Architecture (Logging Strategy):**
- Structured JSON logs via console methods
- Log levels: debug, info, warn, error
- NEVER log sensitive data (sanitize auth tokens)
- Include correlation_id in ALL logs

## Acceptance Criteria

1. **Request Data Extraction**
   - Parses incoming request method (GET, POST, etc.)
   - Extracts URL path and query parameters separately
   - Captures request headers (content-type, authorization redacted)
   - Records request timestamp in ISO-8601 format
   - Stores request body size (bytes)

2. **Response Data Extraction**
   - Captures HTTP status code (200, 400, 401, 500, etc.)
   - Extracts response headers (content-type, x-correlation-id)
   - Records response body size (bytes)
   - Calculates response time from request start to completion
   - Identifies response type (json, html, error, etc.)

3. **Error & Exception Processing**
   - Parses error messages and categorizes by type
   - Extracts error codes (INVALID_PAYLOAD, UNAUTHORIZED, etc.)
   - Captures stack traces (sanitized, no sensitive data)
   - Associates errors with requests via correlation_id
   - Records whether error was client (4xx) or server (5xx)

4. **Timing Data Calculation**
   - Captures CPU execution time in milliseconds
   - Calculates request-to-response latency
   - Measures database query times (when available)
   - Tracks queue processing time
   - Stores all timing with 1ms precision

5. **Endpoint Path Categorization**
   - Categorizes logs by endpoint: /events, /inbox, /inbox/:id/ack, /inbox/:id/retry
   - Standardizes path patterns (/:id becomes template)
   - Supports dashboard filtering by endpoint
   - Tracks metrics per endpoint separately

6. **Status Code Categorization**
   - Groups logs by status: 2xx (success), 4xx (client error), 5xx (server error)
   - Tracks specific codes: 200, 400, 401, 404, 409, 500, 503
   - Enables error rate calculation by status code
   - Identifies trends in failure types

7. **Debug Flag Detection**
   - Detects when debug flags are used (?debug=validation_error, etc.)
   - Tags logs with debug flag type
   - Differentiates intentional errors from real failures
   - Filters debug requests from metrics (optional)

8. **Worker Source Identification**
   - Parses worker_name from tail event context
   - Identifies source: api-worker, queue-consumer, workflow
   - Tags logs with worker version/deployment info
   - Enables per-worker performance analysis

9. **Payload Size Tracking**
   - Calculates request payload size in bytes
   - Records response payload size
   - Tracks total bandwidth usage
   - Identifies oversized requests or responses
   - Supports payload size filtering

10. **Batch Processing Efficiency**
    - Processes multiple logs per batch (100+ logs)
    - Deduplicates identical consecutive logs
    - Compresses log context when possible
    - Stores logs efficiently in D1
    - Batch insert time < 500ms per batch

11. **Sensitive Data Sanitization**
    - Redacts Authorization headers before storage
    - Removes API keys from request/response bodies
    - Strips PII from payloads (email, phone, etc.)
    - Masks token values (show first 4 chars only)
    - Implements regex patterns for data redaction

12. **Log Enrichment**
    - Adds computed fields: endpoint, status_code_class (2xx/4xx/5xx)
    - Includes service version from package.json
    - Adds environment (dev/staging/prod)
    - Enriches with request IP (if available)
    - Calculates derived metrics (requests/sec, errors/sec)

13. **Error Classification**
    - Classifies errors into categories: validation, auth, not_found, conflict, server_error
    - Tracks error frequency by type
    - Identifies patterns (multiple same errors = issue)
    - Enables targeted error monitoring
    - Supports alerting on error categories

14. **Query Optimization**
    - Stores parsed data in columns for efficient filtering
    - Method, path, status_code as indexed columns
    - Timestamp indexed for time-range queries
    - Supports fast aggregations (COUNT, AVG latency, etc.)
    - D1 queries return results in < 100ms

15. **Data Consistency**
    - Ensures all logs follow same structure
    - Validates required fields present
    - Handles missing optional fields gracefully
    - Maintains referential integrity with requests
    - Prevents malformed logs from breaking dashboard

## Dependencies

- **Story 4.1:** Tail Worker must capture raw events
- **Epic 2 Complete:** D1 database and schema available
- **Architecture:** Log structure and parsing patterns defined

## Technical Specifications

### Enhanced D1 Schema for Parsed Logs

```sql
CREATE TABLE log_entries (
  log_id TEXT PRIMARY KEY,
  correlation_id TEXT NOT NULL,
  request_id TEXT,
  timestamp TEXT NOT NULL,

  -- Request info
  method TEXT NOT NULL,              -- GET, POST, etc
  path TEXT NOT NULL,                -- /events, /inbox, etc
  endpoint TEXT NOT NULL,            -- categorized: /events, /inbox, etc
  query_params TEXT,                 -- JSON of query params
  request_headers TEXT,              -- JSON (sanitized)
  request_body_size INTEGER,         -- bytes

  -- Response info
  status_code INTEGER NOT NULL,      -- 200, 400, 500, etc
  status_class TEXT NOT NULL,        -- 2xx, 4xx, 5xx
  response_headers TEXT,             -- JSON (sanitized)
  response_body_size INTEGER,        -- bytes

  -- Timing
  duration_ms INTEGER NOT NULL,      -- total request time
  cpu_ms INTEGER,                    -- CPU execution time
  db_query_ms INTEGER,               -- database time (if applicable)
  queue_wait_ms INTEGER,             -- queue processing time

  -- Error info
  error_code TEXT,                   -- INVALID_PAYLOAD, UNAUTHORIZED, etc
  error_message TEXT,                -- human-readable error
  error_category TEXT,               -- validation, auth, server, etc
  error_stack TEXT,                  -- stack trace (sanitized)

  -- Context
  worker_name TEXT NOT NULL,         -- api-worker, queue-consumer, etc
  debug_flag TEXT,                   -- validation_error, processing_error, etc
  environment TEXT,                  -- dev, staging, prod
  version TEXT,                      -- service version

  -- Metadata
  created_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP,
  processed_at TEXT NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- Indexes for common queries
CREATE INDEX idx_log_timestamp ON log_entries(timestamp DESC);
CREATE INDEX idx_log_correlation ON log_entries(correlation_id);
CREATE INDEX idx_log_endpoint ON log_entries(endpoint);
CREATE INDEX idx_log_status ON log_entries(status_code);
CREATE INDEX idx_log_error ON log_entries(error_category);
CREATE INDEX idx_log_worker ON log_entries(worker_name);
CREATE INDEX idx_log_duration ON log_entries(duration_ms);
CREATE INDEX idx_log_timestamp_endpoint ON log_entries(timestamp DESC, endpoint);
```

### Log Parser Implementation

```typescript
// src/lib/log-parser.ts
import { TailLog } from '../types/logs';

interface ParsedLog {
  log_id: string;
  correlation_id: string;
  request_id: string;
  timestamp: string;
  method: string;
  path: string;
  endpoint: string;
  query_params: Record<string, any>;
  request_headers: Record<string, string>;
  request_body_size: number;
  status_code: number;
  status_class: '2xx' | '4xx' | '5xx';
  response_headers: Record<string, string>;
  response_body_size: number;
  duration_ms: number;
  cpu_ms: number;
  error_code?: string;
  error_message?: string;
  error_category?: string;
  worker_name: string;
  debug_flag?: string;
  environment: string;
  version: string;
}

export class LogParser {
  private sanitizationPatterns = [
    { pattern: /Authorization:\s*Bearer\s+[^\s]+/gi, replacement: 'Authorization: Bearer [REDACTED]' },
    { pattern: /api[_-]?key[:\s=]+[^\s,}]+/gi, replacement: 'api_key: [REDACTED]' },
    { pattern: /token[:\s=]+[^\s,}]+/gi, replacement: 'token: [REDACTED]' },
  ];

  parseTailEvent(rawEvent: any): ParsedLog {
    const logId = crypto.randomUUID();
    const timestamp = new Date(rawEvent.Timestamps?.StartTime).toISOString();
    const correlationId = this.extractCorrelationId(rawEvent);
    const requestId = rawEvent.Outcome?.RequestId || crypto.randomUUID();

    // Parse request
    const { method, path, queryParams, headers: reqHeaders, bodySize } =
      this.parseRequest(rawEvent);
    const endpoint = this.categorizeEndpoint(path);

    // Parse response
    const { statusCode, headers: respHeaders, bodySize: respBodySize } =
      this.parseResponse(rawEvent);
    const statusClass = this.getStatusClass(statusCode);

    // Parse timing
    const { durationMs, cpuMs, dbMs, queueMs } = this.parseTiming(rawEvent);

    // Parse errors
    const { errorCode, errorMessage, errorCategory } = this.parseErrors(rawEvent);

    // Parse debug flags
    const debugFlag = this.extractDebugFlag(path);

    return {
      log_id: logId,
      correlation_id: correlationId,
      request_id: requestId,
      timestamp,
      method,
      path,
      endpoint,
      query_params: queryParams,
      request_headers: this.sanitizeHeaders(reqHeaders),
      request_body_size: bodySize,
      status_code: statusCode,
      status_class: statusClass,
      response_headers: this.sanitizeHeaders(respHeaders),
      response_body_size: respBodySize,
      duration_ms: durationMs,
      cpu_ms: cpuMs,
      db_query_ms: dbMs,
      queue_wait_ms: queueMs,
      error_code: errorCode,
      error_message: errorMessage,
      error_category: errorCategory,
      worker_name: this.getWorkerName(rawEvent),
      debug_flag: debugFlag,
      environment: this.getEnvironment(),
      version: this.getVersion(),
    };
  }

  private parseRequest(event: any): {
    method: string;
    path: string;
    queryParams: Record<string, any>;
    headers: Record<string, string>;
    bodySize: number;
  } {
    // Extract from FetchEvent or raw event data
    const method = event.Request?.Method || 'GET';
    const url = new URL(event.Request?.URL || 'http://localhost/');
    const path = url.pathname;

    const queryParams: Record<string, any> = {};
    url.searchParams.forEach((value, key) => {
      queryParams[key] = value;
    });

    const headers: Record<string, string> = {};
    if (event.Request?.Headers) {
      Object.entries(event.Request.Headers).forEach(([key, value]) => {
        headers[key.toLowerCase()] = String(value);
      });
    }

    const bodySize = parseInt(headers['content-length'] || '0', 10);

    return { method, path, queryParams, headers, bodySize };
  }

  private parseResponse(event: any): {
    statusCode: number;
    headers: Record<string, string>;
    bodySize: number;
  } {
    const statusCode = event.Outcome?.Status || 200;
    const headers: Record<string, string> = {};
    const bodySize = 0; // Would need response size from event

    return { statusCode, headers, bodySize };
  }

  private parseTiming(event: any): {
    durationMs: number;
    cpuMs: number;
    dbMs: number;
    queueMs: number;
  } {
    const cpuMs = event.Outcome?.Cpu || 0;
    const durationMs = event.Outcomes?.Duration || cpuMs;

    return {
      durationMs,
      cpuMs,
      dbMs: 0, // Could extract from structured logs
      queueMs: 0, // Could extract from structured logs
    };
  }

  private parseErrors(event: any): {
    errorCode?: string;
    errorMessage?: string;
    errorCategory?: string;
  } {
    if (!event.Exceptions?.length) {
      return {};
    }

    const exception = event.Exceptions[0];
    const errorMessage = exception.Message || 'Unknown error';
    const errorCode = this.extractErrorCode(errorMessage);
    const errorCategory = this.categorizeError(errorCode, exception);

    return { errorCode, errorMessage, errorCategory };
  }

  private extractCorrelationId(event: any): string {
    // Try to extract from request headers or event context
    // Fallback to generating new UUID
    return event.Outcome?.Logs?.[0]?.correlation_id || crypto.randomUUID();
  }

  private categorizeEndpoint(path: string): string {
    // /inbox/uuid/ack -> /inbox/:id/ack
    return path
      .replace(/\/[a-f0-9\-]{36}/g, '/:id') // UUID pattern
      .replace(/\/\d+/g, '/:id'); // numeric ID pattern
  }

  private getStatusClass(status: number): '2xx' | '4xx' | '5xx' {
    if (status < 300) return '2xx';
    if (status < 400) return '2xx';
    if (status < 500) return '4xx';
    return '5xx';
  }

  private sanitizeHeaders(headers: Record<string, string>): Record<string, string> {
    const sanitized: Record<string, string> = {};

    Object.entries(headers).forEach(([key, value]) => {
      let sanitizedValue = value;

      this.sanitizationPatterns.forEach(({ pattern, replacement }) => {
        sanitizedValue = sanitizedValue.replace(pattern, replacement);
      });

      sanitized[key] = sanitizedValue;
    });

    return sanitized;
  }

  private extractDebugFlag(path: string): string | undefined {
    const match = path.match(/debug=([^&]+)/);
    return match?.[1];
  }

  private extractErrorCode(message: string): string | undefined {
    // Parse error codes from message like "INVALID_PAYLOAD: ..."
    const match = message.match(/^([A-Z_]+):/);
    return match?.[1];
  }

  private categorizeError(errorCode: string | undefined, exception: any): string {
    if (!errorCode) return 'unknown';
    if (errorCode.includes('INVALID')) return 'validation';
    if (errorCode.includes('UNAUTHORIZED')) return 'auth';
    if (errorCode.includes('NOT_FOUND')) return 'not_found';
    if (errorCode.includes('CONFLICT')) return 'conflict';
    return 'server';
  }

  private getWorkerName(event: any): string {
    // Extract from event context or FetchEvent
    return 'api-worker'; // Placeholder
  }

  private getEnvironment(): string {
    return process.env.ENVIRONMENT || 'dev';
  }

  private getVersion(): string {
    // Load from package.json
    return '1.0.0'; // Placeholder
  }
}
```

### Batch Processing Implementation

```typescript
// src/lib/log-batch-processor.ts
export class LogBatchProcessor {
  private logBuffer: ParsedLog[] = [];
  private flushInterval: number = 5000; // 5 seconds
  private maxBatchSize: number = 100;

  constructor(private db: any, private logger: any) {
    this.startPeriodicFlush();
  }

  async addLog(log: ParsedLog): Promise<void> {
    this.logBuffer.push(log);

    if (this.logBuffer.length >= this.maxBatchSize) {
      await this.flush();
    }
  }

  async flush(): Promise<void> {
    if (this.logBuffer.length === 0) return;

    const logsToInsert = [...this.logBuffer];
    this.logBuffer = [];

    try {
      const insertStatement = `
        INSERT INTO log_entries (
          log_id, correlation_id, request_id, timestamp, method, path,
          endpoint, query_params, request_headers, request_body_size,
          status_code, status_class, response_headers, response_body_size,
          duration_ms, cpu_ms, db_query_ms, queue_wait_ms,
          error_code, error_message, error_category, worker_name,
          debug_flag, environment, version
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,
                  ?, ?, ?, ?, ?, ?, ?)
      `;

      for (const log of logsToInsert) {
        await this.db.prepare(insertStatement).bind(
          log.log_id,
          log.correlation_id,
          log.request_id,
          log.timestamp,
          log.method,
          log.path,
          log.endpoint,
          JSON.stringify(log.query_params),
          JSON.stringify(log.request_headers),
          log.request_body_size,
          log.status_code,
          log.status_class,
          JSON.stringify(log.response_headers),
          log.response_body_size,
          log.duration_ms,
          log.cpu_ms,
          log.db_query_ms || null,
          log.queue_wait_ms || null,
          log.error_code || null,
          log.error_message || null,
          log.error_category || null,
          log.worker_name,
          log.debug_flag || null,
          log.environment,
          log.version
        ).run();
      }

      this.logger.info(`Flushed ${logsToInsert.length} logs to D1`);
    } catch (error) {
      this.logger.error('Failed to flush logs', { error: String(error) });
      // Restore buffer and retry (simplified)
      this.logBuffer = [...logsToInsert, ...this.logBuffer];
    }
  }

  private startPeriodicFlush(): void {
    setInterval(() => this.flush(), this.flushInterval);
  }
}
```

## Implementation Workflow

1. **Create Log Parser**
   - Implement LogParser class with methods for request/response/error parsing
   - Add sanitization for sensitive data
   - Add error classification logic

2. **Update D1 Schema**
   - Create log_entries table with parsed columns
   - Add appropriate indexes for query performance
   - Ensure columns align with ParsedLog interface

3. **Implement Batch Processor**
   - Create LogBatchProcessor for efficient D1 inserts
   - Add periodic flush mechanism (5-second interval)
   - Handle backpressure and failures

4. **Integrate with Tail Worker**
   - Update Tail Worker to use LogParser
   - Send parsed logs to batch processor
   - Handle parsing errors gracefully

5. **Add Data Sanitization**
   - Implement regex patterns for auth headers
   - Sanitize API keys and tokens
   - Remove PII from payloads

6. **Test Parsing**
   - Send test requests with various payloads
   - Verify logs appear in log_entries table
   - Check sanitization (no tokens visible)
   - Validate timing calculations

## Verification Checklist

- [ ] log_entries table created with all columns
- [ ] Test request parsed correctly (method, path, status)
- [ ] Authorization headers sanitized
- [ ] Response time calculated accurately
- [ ] Error logs categorized correctly
- [ ] Endpoint categorization working (/inbox/:id/ack, etc)
- [ ] Status code classes correct (2xx/4xx/5xx)
- [ ] Batch insert efficiency (100+ logs per batch)
- [ ] Logs queryable by timestamp, endpoint, status
- [ ] D1 queries return results < 100ms
- [ ] No PII or sensitive data in logs
- [ ] TypeScript types all correct

## Notes

- Log parsing should be fast (< 10ms per log)
- Batch processing optimizes for D1 write efficiency
- Sanitization prevents accidental exposure of credentials
- Endpoint categorization enables per-endpoint metrics
- Error classification enables pattern detection

## Related Stories

- **4.1:** Tail Worker Setup - captures raw events
- **4.3:** Metrics Calculation - uses parsed logs to compute percentiles
- **4.4:** UI Logs Display - queries parsed logs for dashboard
