---
title: "Epic 2.4 - Event Storage: Write to D1 with Status Tracking"
status: "Ready for Development"
epic: "Epic 2: Event Processing & Storage + Metrics Display"
priority: "P0"
acceptance_criteria:
  - "Workflow step 2 executes D1 INSERT with complete event record"
  - "Event stored with 7 fields: event_id, payload, metadata, status, created_at, updated_at, retry_count"
  - "Status field initialized as 'pending' for all new events"
  - "created_at captures original event ingestion timestamp"
  - "updated_at captures workflow execution timestamp"
  - "retry_count reflects current attempt number (0 for first attempt)"
  - "Payload and metadata serialized as JSON strings before storage"
  - "NULL metadata accepted and stored correctly"
  - "UNIQUE PRIMARY KEY on event_id prevents duplicates"
  - "Insert operation completes within 100ms on typical load"
  - "Transaction safety: INSERT succeeds completely or fails completely"
  - "Error handling: Duplicate event_id returns specific error (UNIQUE constraint)"
  - "Concurrent inserts from multiple workflows handled correctly"
  - "D1 query returns all fields with correct types (TEXT, JSON, INTEGER)"
  - "Event retrieval via GET /inbox returns stored payload correctly"
created_at: "2025-11-10"
modified_at: "2025-11-10"
story_size: "Medium"
depends_on: "Epic 1.1 - Project Setup, Epic 2.1 - D1 Schema, Epic 2.3 - Workflow"
---

## Summary

Implement the durable event storage step in the workflow. This story focuses on the D1 database write operation that persists events with status tracking, ensuring all ingested events are recorded with proper metadata for downstream retrieval and management.

## Business Value

Guarantees event persistence. Without reliable storage, events are lost. This ensures that every event accepted via the API is durable stored with tracking for subsequent retrieval, status management, and analytics.

## Technical Requirements

### Event Storage Interface

**From architecture.md - D1 Schema:**

```typescript
interface StoredEvent {
  event_id: string;           // UUID
  payload: Record<string, any>; // JSON
  metadata?: Record<string, any>; // JSON or NULL
  status: 'pending' | 'delivered' | 'failed';
  created_at: string;         // ISO-8601 timestamp
  updated_at: string;         // ISO-8601 timestamp
  retry_count: number;        // 0 on first attempt, increments on retries
}
```

### D1 Storage Operation

**File Location:** `src/db/queries.ts` (createEvent method, already stubbed in 2.1)

**Complete Implementation:**

```typescript
import { D1Database } from '@cloudflare/workers-types';
import { Event, CreateEventInput } from '../types/events';

export class EventQueries {
  constructor(private db: D1Database) {}

  /**
   * Store a new event in D1 with status tracking
   * Called from workflow step 2
   */
  async createEvent(
    event_id: string,
    payload: Record<string, any>,
    metadata: Record<string, any> | undefined,
    timestamp: string,
    retryCount: number = 0,
  ): Promise<Event> {
    const now = new Date().toISOString();

    try {
      const result = await this.db
        .prepare(`
          INSERT INTO events (
            event_id,
            payload,
            metadata,
            status,
            created_at,
            updated_at,
            retry_count
          ) VALUES (?, ?, ?, ?, ?, ?, ?)
          RETURNING *
        `)
        .bind(
          event_id,                              // TEXT PRIMARY KEY
          JSON.stringify(payload),               // JSON → TEXT
          metadata ? JSON.stringify(metadata) : null, // JSON or NULL
          'pending',                             // status: pending
          timestamp,                             // created_at: original ingestion time
          now,                                   // updated_at: current time
          retryCount,                            // retry_count: 0 on first attempt
        )
        .first<Event>();

      if (!result) {
        throw new Error('Failed to retrieve inserted event');
      }

      return {
        ...result,
        // Parse JSON strings back to objects for application layer
        payload: typeof result.payload === 'string'
          ? JSON.parse(result.payload)
          : result.payload,
        metadata: result.metadata
          ? (typeof result.metadata === 'string'
              ? JSON.parse(result.metadata)
              : result.metadata)
          : undefined,
      };
    } catch (error) {
      if (error instanceof Error && error.message.includes('UNIQUE constraint')) {
        throw new Error(`Duplicate event_id: ${event_id}`);
      }
      throw error;
    }
  }

  /**
   * Get event by ID for verification
   */
  async getEvent(eventId: string): Promise<Event | null> {
    const result = await this.db
      .prepare('SELECT * FROM events WHERE event_id = ?')
      .bind(eventId)
      .first<StoredEvent>();

    if (!result) return null;

    return this.parseEventFromDb(result);
  }

  /**
   * Update event status (pending → delivered or failed)
   */
  async updateEventStatus(
    eventId: string,
    newStatus: 'pending' | 'delivered' | 'failed',
  ): Promise<void> {
    const now = new Date().toISOString();

    await this.db
      .prepare('UPDATE events SET status = ?, updated_at = ? WHERE event_id = ?')
      .bind(newStatus, now, eventId)
      .run();
  }

  /**
   * Increment retry count on failed processing attempts
   */
  async incrementRetryCount(eventId: string): Promise<void> {
    const now = new Date().toISOString();

    await this.db
      .prepare(`
        UPDATE events
        SET retry_count = retry_count + 1, updated_at = ?
        WHERE event_id = ?
      `)
      .bind(now, eventId)
      .run();
  }

  /**
   * Helper: Parse event from database (parse JSON strings)
   */
  private parseEventFromDb(row: StoredEvent): Event {
    return {
      ...row,
      payload: typeof row.payload === 'string'
        ? JSON.parse(row.payload)
        : row.payload,
      metadata: row.metadata
        ? (typeof row.metadata === 'string'
            ? JSON.parse(row.metadata)
            : row.metadata)
        : undefined,
    };
  }
}
```

### Workflow Integration (Update to 2.3)

**File:** `src/workflows/process-event.ts`

Storage step using EventQueries:

```typescript
// Step 2: Store event in D1
const stored = await step.do(
  'store-event',
  async () => {
    logger.debug('Storing event to D1', {
      correlation_id,
      event_id,
    });

    const queries = new EventQueries(env.DB);

    try {
      const storedEvent = await queries.createEvent(
        event_id,
        payload,
        metadata,
        timestamp,
        retry_attempt,
      );

      logger.info('Event stored successfully', {
        correlation_id,
        event_id,
        status: storedEvent.status,
        stored_at: storedEvent.updated_at,
      });

      return {
        event_id: storedEvent.event_id,
        status: storedEvent.status,
        stored_at: storedEvent.updated_at,
      };
    } catch (error) {
      logger.error('Failed to store event', {
        correlation_id,
        event_id,
        error: error instanceof Error ? error.message : 'Unknown',
      });
      throw error; // Workflow retry
    }
  },
);
```

### Status Lifecycle

**Event Status Transitions:**

```
┌─────────────────────────────────────────────┐
│  Event Ingestion (API POST)                 │
└────────────────┬────────────────────────────┘
                 │ Event queued
                 ▼
┌─────────────────────────────────────────────┐
│  Queue Processing (Consumer)                │
└────────────────┬────────────────────────────┘
                 │ Workflow invoked
                 ▼
┌─────────────────────────────────────────────┐
│  Workflow Step 1: Validate                  │
└────────────────┬────────────────────────────┘
                 │ Success
                 ▼
┌─────────────────────────────────────────────┐
│  Workflow Step 2: Store (THIS STORY)        │
│  Status = 'pending'                         │
│  created_at = original timestamp            │
│  updated_at = storage timestamp             │
└────────────────┬────────────────────────────┘
                 │ Event now durable in D1
                 ▼
┌─────────────────────────────────────────────┐
│  Workflow Step 3: Update Metrics            │
│  Increment KV counters                      │
└────────────────┬────────────────────────────┘
                 │ Workflow complete
                 ▼
┌─────────────────────────────────────────────┐
│  Event Available for Retrieval (GET /inbox) │
│  Status: pending (awaiting delivery)        │
└─────────────────────────────────────────────┘
```

### Timestamp Handling

**created_at:** Captured at API ingestion time
- Preserved through queue and workflow
- Represents "when was this event sent to the system"
- Used for audit trail and sorting

**updated_at:** Updated on each status change
- Initially set to storage timestamp
- Updated again if status changes (pending → delivered/failed)
- Tracks "last modification" timestamp

### Data Type Handling

**JSON Serialization:**

```typescript
// Application receives: Record<string, any>
const payload = { user_id: "123", action: "login" };

// Database stores: TEXT (JSON string)
const stored = JSON.stringify(payload);
// → '{"user_id":"123","action":"login"}'

// Query returns: TEXT
const retrieved = '{"user_id":"123","action":"login"}';

// Application parses: Record<string, any>
const parsed = JSON.parse(retrieved);
// → { user_id: "123", action: "login" }
```

### Error Handling

**Duplicate Event (UNIQUE Constraint):**

```typescript
// If same event_id inserted twice:
// D1 rejects with: Error: UNIQUE constraint failed: events.event_id

// Workflow handles:
// 1. Catch constraint error
// 2. Log specific error type
// 3. Return failure (could also be idempotent: return success)
// 4. For now: treat as failure → DLQ
```

**NULL Handling:**

```typescript
// metadata is optional
const event1 = {
  payload: { data: "required" },
  metadata: { source: "api" }
};

const event2 = {
  payload: { data: "required" },
  metadata: undefined
};

// Both insert successfully:
// event1 → metadata = '{"source":"api"}'
// event2 → metadata = NULL
```

### Performance Characteristics

**Insert Performance Targets:**
- Single event insert: < 50ms
- Batch of 100 events: < 100ms per event
- Concurrent inserts: Linear scaling up to D1 write limits

**Database Constraints:**
- D1 SQLite single-writer (Cloudflare handles queuing)
- Multiple concurrent writes queue behind single writer
- Eventually completes but may increase latency under high load

**Optimization:**
- Index on event_id (PRIMARY KEY) - instant lookup
- Composite index on (status, created_at) - efficient queries
- No full-text search or complex joins needed

### Concurrency & Atomicity

**Transaction Boundaries:**

```sql
-- Single INSERT is atomic transaction
-- Either succeeds completely or fails completely
-- No partial state possible

INSERT INTO events (...)
VALUES (...);

-- Can't have: event_id stored but payload missing
-- Either full row inserted or entire operation fails
```

**Concurrent Scenarios:**

```typescript
// Scenario 1: Two workflows process same event_id (race condition)
// Result: Second INSERT fails → UNIQUE constraint
// Workflow handles: Log error, return failure, route to DLQ

// Scenario 2: Multiple different events inserted concurrently
// Result: All succeed (D1 queues writes, no conflict)
// Performance: May see increased latency if truly concurrent

// Scenario 3: INSERT fails due to D1 connectivity
// Result: Workflow catches error
// Workflow handles: Retry logic (Workflow step retry)
```

### Testing Strategy

**Unit Testing - Query Functions:**

```typescript
// test/db/queries.test.ts

describe('EventQueries', () => {
  it('creates event with correct fields', async () => {
    const queries = new EventQueries(mockDb);
    const event = await queries.createEvent(
      'event-123',
      { user_id: '456' },
      { source: 'test' },
      '2025-11-10T12:00:00Z',
      0,
    );

    expect(event.event_id).toBe('event-123');
    expect(event.status).toBe('pending');
    expect(event.payload).toEqual({ user_id: '456' });
    expect(event.metadata).toEqual({ source: 'test' });
  });

  it('handles NULL metadata', async () => {
    const event = await queries.createEvent(
      'event-456',
      { data: 'test' },
      undefined,
      '2025-11-10T12:00:00Z',
      0,
    );

    expect(event.metadata).toBeUndefined();
  });

  it('rejects duplicate event_id', async () => {
    await queries.createEvent('event-789', { data: 'first' }, undefined, '...', 0);

    await expect(
      queries.createEvent('event-789', { data: 'duplicate' }, undefined, '...', 0),
    ).rejects.toThrow('Duplicate event_id');
  });
});
```

**Integration Testing - Workflow Step:**

```bash
# Manual: Send event via API
curl -X POST http://localhost:8787/events \
  -H "Authorization: Bearer test-token" \
  -H "Content-Type: application/json" \
  -d '{"payload":{"test":"data"}}'

# Verify in D1
npx wrangler d1 execute triggers-api --local
SELECT * FROM events WHERE status = 'pending';

# Verify JSON parsed correctly
# Verify created_at matches ingestion time
# Verify updated_at after storage
```

### Debugging Storage Issues

**Check Stored Event:**

```sql
-- Raw database view
SELECT
  event_id,
  payload,        -- JSON text
  metadata,       -- JSON text or NULL
  status,
  created_at,
  updated_at,
  retry_count
FROM events
WHERE event_id = 'test-event-id';
```

**Verify Schema:**

```sql
-- Check table structure
PRAGMA table_info(events);

-- Check indexes
PRAGMA index_list(events);

-- Test INSERT
INSERT INTO events
  (event_id, payload, metadata, status, created_at, updated_at, retry_count)
VALUES
  ('manual-test', '{}', NULL, 'pending', '2025-11-10T12:00:00Z', '2025-11-10T12:00:00Z', 0);

SELECT * FROM events WHERE event_id = 'manual-test';
```

---

## Implementation Notes

### What Gets Done

1. Complete `src/db/queries.ts` EventQueries class with full CRUD methods
2. Implement createEvent() with JSON serialization and error handling
3. Implement getEvent(), updateEventStatus(), incrementRetryCount()
4. Add parseEventFromDb() helper for JSON deserialization
5. Create comprehensive test: `test/db/queries.test.ts`
6. Update EventQueries types: ensure proper type safety
7. Integrate with workflow: Use EventQueries in workflow step 2
8. Test locally: Send events and verify storage
9. Commit: `git add src/db/ && git commit -m "feat: D1 event storage with status tracking"`

### Development Workflow

1. Ensure D1 schema created (Epic 2.1)
2. Start: `npx wrangler dev`
3. Send test event via API curl
4. Query D1 to verify storage
5. Check timestamps and retry count
6. Test duplicate event handling
7. Test NULL metadata

### Key Architecture Decisions

**JSON Serialization:** Store payloads as JSON text for flexibility

**Status Tracking:** Initialize as 'pending' for all new events

**Timestamp Preservation:** created_at from ingestion, updated_at from storage

**Error Handling:** Duplicate event_id treated as error (or could be idempotent)

---

## Acceptance Criteria Verification Checklist

### Event Storage
- [ ] createEvent() inserts all 7 fields correctly
- [ ] event_id field stored as TEXT PRIMARY KEY
- [ ] payload field serialized to JSON string
- [ ] metadata field nullable, serialized if present
- [ ] status field always 'pending' for new events
- [ ] created_at field contains original ingestion timestamp
- [ ] updated_at field contains storage timestamp
- [ ] retry_count field contains attempt number (0 on first)

### Data Types
- [ ] event_id: TEXT stored as string UUID
- [ ] payload: JSON strings parsed back to objects
- [ ] metadata: JSON strings or NULL, parsed back correctly
- [ ] status: TEXT in ('pending', 'delivered', 'failed')
- [ ] created_at: ISO-8601 timestamp string
- [ ] updated_at: ISO-8601 timestamp string
- [ ] retry_count: INTEGER

### Transaction Safety
- [ ] INSERT succeeds completely or fails completely
- [ ] No partial state possible
- [ ] UNIQUE constraint prevents duplicate event_ids
- [ ] Duplicate event_id error message is specific

### Performance
- [ ] Single event insert < 50ms
- [ ] Batch 100 events in < 100ms per event average
- [ ] Concurrent inserts scale linearly
- [ ] Index on event_id enables fast lookups

### Error Handling
- [ ] Duplicate event_id caught and logged
- [ ] NULL metadata handled correctly
- [ ] D1 connectivity errors logged and retried
- [ ] Error messages are specific and actionable

### Workflow Integration
- [ ] EventQueries called from workflow step 2
- [ ] Stored event returned with all fields
- [ ] Failure logged with correlation_id
- [ ] Failure triggers workflow retry

### Query Methods
- [ ] getEvent() retrieves stored event
- [ ] getEvent() parses JSON back to objects
- [ ] getEvent() returns NULL for non-existent ID
- [ ] updateEventStatus() changes status field
- [ ] updateEventStatus() updates updated_at timestamp
- [ ] incrementRetryCount() increments counter
- [ ] All query methods type-safe (no `any` types)

---

## Dependencies & Context

**From:** docs/PRD.md (Epic 2 section - Durable Storage FR-2.3)
**Architecture:** docs/architecture.md (Data Architecture - D1 Schema)
**Depends On:** Epic 1.1 (Project Setup), Epic 2.1 (D1 Schema), Epic 2.3 (Workflow)
**Enables:** Epic 2.5 (Metrics), Epic 3.1 (Inbox Query)

---

## Dev Notes

- D1 is managed SQLite - use `.prepare()` for parameterized queries (prevents SQL injection)
- JSON serialization/deserialization must be bidirectional (parse on retrieval)
- created_at should be set once (never updated), updated_at updates on status changes
- RETURNING * clause ensures we get back the inserted row for validation
- Retry count reflects workflow step attempt number (0 on first, increments on retries)
- Null metadata is valid - don't require it

---
